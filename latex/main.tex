% !TeX program = xelatex
\documentclass[12pt,hyperref,a4paper,UTF8]{ctexart}
\usepackage{SJTUReport}
\usepackage{booktabs}  % 在导言区添加
\usepackage[commandnameprefix=always]{changes} % final 可以替换为 draft 查看标注效果
\setaddedmarkup{\textcolor{red}{#1}} % 设置新增内容为红色
\setdeletedmarkup{\textcolor{red}{\sout{#1}}} % 设置删除内容为红色并加删除线
\let\comment\chcomment
\usepackage{listings}
\usepackage{xcolor}

% 自定义样式（可选，重点配置）
\lstset{
    language=C++,           % 指定语言
    basicstyle=\ttfamily\small,  % 基础字体：等宽+小号
    keywordstyle=\color{blue}\bfseries,  % 关键字：蓝色+粗体
    commentstyle=\color{gray}\itshape,   % 注释：灰色+斜体
    stringstyle=\color{red},            % 字符串：红色
    numbers=left,            % 行号显示在左侧
    numberstyle=\tiny\color{gray},  % 行号样式：小号+灰色
    frame=single,            % 单框包围代码
    frameround=ffff,         % 边框圆角（可选）
    breaklines=true,         % 自动换行（解决长行溢出）
    showspaces=false,        % 不显示空格符号
    showtabs=false,          % 不显示Tab符号
    tabsize=4,               % Tab缩进为4个字符
    escapeinside={/*@}{@*/}  % 可选：允许代码中嵌入LaTeX命令
}
% 加载图片处理包（支持png/jpg/pdf等格式）
\usepackage{graphicx}
% 可选：设置图片根目录（简化路径，推荐）
\graphicspath{{figures/}} % 表示所有图片默认从figures文件夹读取
\usepackage{tabularx}  % 自适应列宽，解决长文本溢出
\usepackage{booktabs}  % 美化表格横线（替代竖线，学术更规范）
\usepackage{siunitx}     % 数值小数位对齐（核心）
\usepackage{ragged2e}  % 优化单元格文本对齐
\usepackage{multirow}  % 因为用了\multirow
% 定义自适应列：左对齐+自动换行
\newcolumntype{Y}{>{\RaggedRight\arraybackslash}X}
% 定义数值列：保留2位小数，居中对齐
\newcolumntype{F}[1]{>{\centering\arraybackslash}S[table-format=#1, round-mode=places, round-precision=2]}

%%-------------------------------正文开始---------------------------%%
\begin{document}

%%-----------------------封面--------------------%%
\cover

%%------------------摘要-------------%%
%\begin{abstract}
%
%在此填写摘要内容
%
%\end{abstract}

\thispagestyle{empty} % 首页不显示页码

%%--------------------------目录页------------------------%%
\newpage
\tableofcontents

%%------------------------正文页从这里开始-------------------%
\newpage

%%可选择这里也放一个标题
%\begin{center}
%    \title{ \Huge \textbf{{标题}}}
%\end{center}

\section{引言：并行计算基础}

\subsection{并行计算概述}

并行计算是指同时使用多个计算资源解决计算问题的过程。随着处理器主频增长放缓，多核处理器成为提升计算性能的主要途径。通过将任务分解为可以并发执行的子任务，并行计算能够显著缩短程序运行时间，提高计算效率。

\subsubsection{并行计算的分类}

根据并行粒度的不同，并行计算可分为：

\begin{itemize}
    \item \textbf{任务级并行（Task-Level Parallelism）}：将不同的任务分配给不同的处理器执行
    \item \textbf{数据级并行（Data-Level Parallelism）}：对大规模数据集的不同部分同时进行相同操作
    \item \textbf{指令级并行（Instruction-Level Parallelism）}：在单个处理器内同时执行多条指令
\end{itemize}

本项目主要关注数据级并行，利用OpenMP实现共享内存多线程并行。

\subsection{OpenMP并行编程模型}

OpenMP（Open Multi-Processing）是一种支持共享内存并行编程的API，广泛应用于C/C++和Fortran程序中。其主要特点包括：

\begin{itemize}
    \item \textbf{Fork-Join模型}：主线程创建多个工作线程，并行执行后再汇合
    \item \textbf{编译器指令}：通过\texttt{\#pragma omp}指令控制并行行为
    \item \textbf{可移植性强}：支持多种编译器和操作系统
    \item \textbf{增量并行化}：可逐步对串行程序进行并行优化
\end{itemize}

常用的OpenMP并行指令包括：
\begin{itemize}
    \item \texttt{\#pragma omp parallel for}：并行执行for循环
    \item \texttt{\#pragma omp parallel sections}：并行执行不同的代码段
    \item \texttt{\#pragma omp critical}：保护临界区
    \item \texttt{\#pragma omp barrier}：设置同步点
\end{itemize}


\begin{figure}[!htbp]
    \centering
    \includegraphics[width =.8\textwidth]{fork-join.png}
    \caption{OpenMP Fork-Join模型示意图}
    \label{fig:openmp_fork_join}
\end{figure}

\subsection{性能评估指标}

\subsubsection{加速比（Speedup）}

加速比是衡量并行性能最常用的指标，定义为：

\begin{equation}
    S_p = \frac{T_1}{T_p}
    \label{eq:speedup}
\end{equation}

其中$T_1$为串行执行时间，$T_p$为使用$p$个处理器的并行执行时间。理想情况下$S_p = p$（线性加速），但实际中通常$S_p < p$。

\subsubsection{并行效率（Efficiency）}

并行效率衡量处理器资源的利用率：

\begin{equation}
    E_p = \frac{S_p}{p} = \frac{T_1}{p \cdot T_p}
    \label{eq:efficiency}
\end{equation}

理想情况下$E_p = 1$（100\%），表示所有处理器都被充分利用。

\subsubsection{Amdahl定律}

Amdahl定律描述了程序中串行部分对并行加速比的限制：

\begin{equation}
    S_p \leq \frac{1}{f + \frac{1-f}{p}}
    \label{eq:amdahl}
\end{equation}

其中$f$是程序中不可并行部分的比例。当$p \to \infty$时，$S_p \to \frac{1}{f}$。这说明即使有无限多处理器，加速比也受串行部分的限制。


\subsection{Roofline模型}

Roofline模型是分析程序性能的重要工具，它描述了性能上限由两个因素决定：

\begin{equation}
    \text{Performance} = \min(\text{Peak FLOPS}, \text{Arithmetic Intensity} \times \text{Memory Bandwidth})
    \label{eq:roofline}
\end{equation}


\begin{figure}[!htbp]
    \centering
    \includegraphics[width =.5\textwidth]{roofline.png}
    \caption{Roofline模型示意图}
    \label{fig:roofline_model}
\end{figure}
图中的 $I_{\text{max}}$ 为拐点所对应的强度，是计算平台的计算强度上限。当 $I < I_{\text{max}}$ 时，模型的计算性能受限于带宽大小，处于带宽瓶颈区；当 $I > I_{\text{max}}$ 时，模型的计算性能瓶颈取决于平台的算力，处于计算瓶颈区。所以，模型的计算强度应尽量大于 $I_{\text{max}}$，这样才能最大程度利用计算平台的算力资源，但超过之后就无需追求无意义的提升，因为再强性能都只能达到计算平台的算力。



\subsection{性能benchmark与测试环境}

本项目的所有性能测试均在以下环境下进行：

\begin{itemize}
    \item \textbf{处理器}：AMD Ryzen AI 9 H 365 w/ Radeon 880M（10核心20线程，基频2.0GHz）
    \item \textbf{内存}：32GB
    \item \textbf{编译器}：GCC 15.2.0 (MinGW)，Clang 17.0.6
    \item \textbf{操作系统}：Windows 11 家庭中文版
    \item \textbf{并行库}：OpenMP
\end{itemize}

性能测试方法：
\begin{itemize}
    \item 测试算子时先50次warm up，再200次取中位数和P99
    \item 使用高精度计时器（\texttt{std::chrono::high\_resolution\_clock}）
    \item 测试前进行warm-up避免冷启动影响
\end{itemize}

\clearpage
\section{神经网络算子并行化}

神经网络中的算子计算通常具有高度的数据并行性，通过OpenMP等并行技术可以显著提升计算性能。本节介绍两个典型的神经网络算子：卷积（Convolution）和平均池化（Average Pooling）的并行实现。
\subsection{计算密集型 vs 访存密集型} 


在并行计算中，算子的性能特征可以分为两类：

\subsubsection{计算密集型（Compute-Bound）}
计算密集型任务的核心定义是算法的性能瓶颈在于计算单元（如ALU、FPU）的吞吐量，而非内存带宽。这类任务具有高计算访存比，即每次内存访问都会对应大量计算操作，算术强度显著偏高，浮点运算次数与内存访问字节数的比值远大于1，其性能瓶颈主要集中在处理器的浮点运算能力，因此在并行处理场景中通常能获得较好的加速比。

计算密集型任务的典型例子包括卷积操作和矩阵乘法，其中卷积操作以输入$(1,3,150,150)$、输出$(1,32,150,150)$、步长stride=1、填充padding=2、卷积核大小kernel\_size=$(5,5)$为例，算术强度约为386.27 FLOP/byte；而矩阵乘法对于$n \times n$规模的矩阵，算术强度约为$\frac{n}{3}$ FLOP/byte，且随着$n$的增大，算术强度的特征会更加显著。针对计算密集型任务的优化策略主要包括增加并行度，比如通过多线程、SIMD技术提升并行处理能力，同时可通过提高指令级并行、循环展开等方式进一步挖掘性能潜力。

计算密集型任务的并行加速比接近线性，其主要挑战在于实现负载均衡，避免部分计算单元处于空闲状态，优化重点始终围绕增加计算并行度展开，性能特征上则接近处理器的峰值浮点运算性能（Peak FLOPS），对应性能屋顶模型中的水平部分。

\subsubsection{访存密集型（Memory-Bound）}
访存密集型任务与计算密集型任务形成鲜明对比，其性能瓶颈在于内存系统的带宽，而非处理器的计算能力。这类任务的计算访存比偏低，每次内存访问仅对应少量计算操作，算术强度较小，浮点运算次数与内存访问字节数的比值远小于1，性能表现主要受限于内存带宽大小和缓存命中率高低，在并行处理场景中，由于内存带宽的限制，加速比提升空间会受到明显约束。

访存密集型任务的典型例子包括池化操作、Batch Normalization以及ReLU等激活函数，其中$2 \times 2$规模的池化操作算术强度约为0.31 FLOP/byte，Batch Normalization操作主要以内存读写为主，仅伴随少量简单运算，而ReLU等激活函数多为逐元素操作，计算量极小，核心开销集中在内存访问过程中。针对这类任务的优化策略需围绕内存访问效率展开，比如通过数据重用、分块处理等方式提高缓存命中率，减少不必要的内存访问次数，同时可通过预取优化、调整内存访问模式等手段进一步降低内存访问延迟、提升内存带宽利用率。

访存密集型任务的并行加速比明显受限，核心挑战在于多线程并行时的内存带宽竞争，多个线程同时访问内存会导致带宽资源紧张，反而可能出现性能下降的情况，因此优化重点在于减少内存访问总量、提高缓存利用效率。其性能特征严格受限于内存带宽，无法充分发挥处理器的计算能力，对应性能屋顶模型中的斜线部分，即性能随内存带宽的提升而增长，却难以突破内存带宽带来的上限约束。
\subsection{卷积算子(Conv2d) - 计算密集型}

\subsubsection{算法原理}

二维卷积是深度学习中最基础且最重要的操作之一。给定输入特征图$X \in \mathbb{R}^{C_{in} \times H_{in} \times W_{in}}$和卷积核$W \in \mathbb{R}^{C_{out} \times C_{in} \times K_h \times K_w}$，卷积操作计算输出特征图$Y \in \mathbb{R}^{C_{out} \times H_{out} \times W_{out}}$：

\begin{equation}
    Y[o, h, w] = b[o] + \sum_{c=0}^{C_{in}-1} \sum_{i=0}^{K_h-1} \sum_{j=0}^{K_w-1} X[c, h \cdot s_h + i, w \cdot s_w + j] \cdot W[o, c, i, j]
    \label{eq:conv2d}
\end{equation}

其中$C_{in}$, $C_{out}$为输入和输出通道数，$K_h$, $K_w$为卷积核高度和宽度，$s_h$, $s_w$为步长，$b[o]$为偏置项。输出特征图的尺寸计算公式：

\begin{equation}
    H_{out} = \left\lfloor \frac{H_{in} + 2 \cdot \text{padding} - K_h}{s_h} \right\rfloor + 1
    \label{eq:conv_output_size}
\end{equation}

\subsubsection{实现策略演进}

本项目实现了三个版本的卷积算子，展示了从串行到并行、再到深度优化的演进过程。

\textbf{版本1：串行实现} - 使用动态计数器记录输出位置，逻辑简单但存在数据依赖，无法直接并行。

\autoref{lst:conv_serial}展示了该版本的核心代码。

\textbf{问题分析：}\texttt{cnt[c]++}操作存在数据竞争而无法直接并行化，\texttt{weight\_pos}采用循环累加的实现方式导致存在跨线程依赖，同时索引计算逻辑复杂也显著降低了执行性能。\\
\\
\textbf{版本2：并行实现} - 改为静态索引计算，在输出通道维度并行，消除依赖但并行粒度仅32。

\autoref{lst:conv_naive}展示了该版本的核心代码。

\textbf{问题分析：}
索引计算逻辑仍较为复杂，导致计算开销偏高；相同索引被多次重复计算，进一步增加了性能损耗；此外，代码还可能存在伪共享（False Sharing）问题，影响并行执行效率。\\
\\
\textbf{版本3：深度优化实现} - 针对版本2的瓶颈实现三方面优化：

\autoref{lst:conv_optimized}展示了该版本的核心代码。

如\autoref{tab:parallel_perf}所示，三个版本的性能对比清晰展现了优化效果。

\begin{table}[htbp]
    \centering
    \caption{普通并行 vs 优化后并行性能对比}
    \label{tab:parallel_perf}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{c|ccc|ccc|c}
        \toprule
        \multirow{2}{*}{\textbf{线程数}} 
        & \multicolumn{3}{c|}{\textbf{普通并行}} 
        & \multicolumn{3}{c|}{\textbf{优化后并行}} 
        & \multirow{2}{*}{\textbf{性能提升率}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7}
        & \textbf{中位数时间(ms)} 
        & \textbf{加速比} 
        & \textbf{效率(\%)} 
        & \textbf{中位数时间(ms)} 
        & \textbf{加速比} 
        & \textbf{效率(\%)} 
        & \textbf{(vs 普通并行)} \\
        \midrule
        1  & 18.20 & 1.00 & 100.00 & 18.58 & 1.00 & 100.00 & -2.1\% \\
        2  & 9.88  & 1.84 & 91.90  & 9.32  & 1.99 & 99.50  & +5.7\% \\
        4  & 5.52  & 3.30 & 82.54  & 4.93  & 3.77 & 94.25  & +10.7\% \\
        8  & 4.01  & 4.54 & 56.72  & 3.72  & 5.00 & 62.50  & +7.2\% \\
        10 & 4.05  & 4.49 & 44.91  & 2.82  & 6.59 & 65.90  & +30.4\% \\
        16 & 3.63  & 5.01 & 31.32  & 2.65  & 7.00 & 43.75  & +27.0\% \\
        20 & 3.54  & 5.14 & 25.68  & 2.36  & 7.86 & 39.30  & \textbf{+33.3\%} \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\subsubsection{并行化挑战与优化}

\textbf{1. 并行粒度选择} - 如\autoref{tab:conv_parallel_granularity}所示，不同并行维度性能差异巨大。

\begin{table}[!htbp]
    \centering
    \caption{卷积不同并行粒度的性能对比}
    \label{tab:conv_parallel_granularity}
    \begin{tabular}{c|c|c|c|c}
    \hline
        并行维度 & 并行数 & 每线程计算量 & 开销占比 & 推荐度 \\
        \hline
        width(w) & 150 & $\sim$600 FLOP & >99\% &  极差 \\
        height(h) & 150 & $\sim$90K FLOP & $\sim$10\% &  较差 \\
        输入通道(ic) & 3 & $\sim$16M FLOP & <0.1\% &  一般 \\
        \textbf{输出通道(oc)} & \textbf{32} & \textbf{$\sim$1.7M FLOP} & \textbf{<1\%} & \textbf{ 最佳} \\
        \hline
    \end{tabular}
\end{table}

\textbf{关键发现：}width并行灾难性失败（20线程反而慢69×），原因是线程开销远大于计算时间；输出通道并行表现最佳；二维空间并行进一步优化（加速7.86×）。

\textbf{2. False Sharing} - 解决方案：按输出通道并行或二维空间并行，每线程写入独立位置。

\textbf{3. 内存带宽限制} - 算术强度约386.27 FLOP/byte，虽属计算密集型，但多线程时仍可能受带宽限制。这是由于本次测试用的张量维数较低，计算量不够大，对于现代CPU来说，瓶颈仍为内存访问。

\subsection{平均池化算子(AvgPool2d) - 访存密集型}

\subsubsection{算法原理}

平均池化对输入特征图的局部区域求平均，用于降采样。给定输入$X \in \mathbb{R}^{C \times H \times W}$和池化窗口$(K_h, K_w)$，输出$Y \in \mathbb{R}^{C \times H' \times W'}$：

\begin{equation}
    Y[c, h, w] = \frac{1}{K_h \times K_w} \sum_{i=0}^{K_h-1} \sum_{j=0}^{K_w-1} X[c, h \cdot s_h + i, w \cdot s_w + j]
    \label{eq:avgpool}
\end{equation}

\subsubsection{并行实现与性能}
\textbf{版本1：普通并行} - 仅添加\texttt{\#pragma omp parallel for}，在通道维度并行。

\autoref{lst:avgpool_naive}展示了该版本的核心代码。

\textbf{版本2：访存优化} - 通过预计算通道指针、优化边界检查和针对2x2池化的特殊优化，提升内存访问效率。

\autoref{lst:avgpool_optimized}展示了该版本的核心代码。

池化操作属典型访存密集型算子，在通道维度并行，使用连续内存访问模式提高缓存命中率。


\begin{table}[htbp]  % 浮动体位置参数，适配前文讲解的htbp
    \centering
    \caption{两种池化实现对比}  % 表格标题（按需修改）
    \label{tab:pool_optim}     % 交叉引用标签（按需修改）
    \resizebox{\linewidth}{!}{% 自适应页面宽度
        \begin{tabularx}{\linewidth}{cYYc}
            \toprule  % 顶部粗线
            \textbf{优化维度} & \textbf{普通版本} & \textbf{访存优化版本} & \textbf{性能影响} \\
            \midrule  % 中间细线
            \textbf{指针运算} & 每次循环重复计算全局索引 \texttt{(d*C*H*W + c*H*W + h*W + w)} & \textbf{预计算通道指针} \texttt{input\_channel\_ptr} 和 \texttt{output\_channel\_ptr} & \textbf{高} 消除大量乘法运算 \\
            \midrule
            \textbf{边界检查} & 内层循环逐像素\texttt{if(h < input\_h \&\& w < input\_w)} & \textbf{外提边界判断}，内层循环无分支 & \textbf{高} 减少分支预测失败 \\
            \midrule
            \textbf{特殊Case特化} & 通用循环处理所有kernel size & \textbf{2×2 kernel stride=2 手动展开} & \textbf{极高} 向量化+指令级并行 \\
            \midrule
            \textbf{除法优化} & \texttt{sum / count} 浮点除法 & 2×2 case用 \texttt{* 0.25f} 乘法代替 & \textbf{中} 除法耗时是乘法的5-10倍 \\
            \midrule
            \textbf{访存连续性} & 跨通道访问导致跳跃 & \textbf{通道内连续访存} + 局部性优化 & \textbf{高} 提升Cache命中率 \\
            \bottomrule  % 底部粗线
        \end{tabularx}
    }
\end{table}
\begin{table}[!htbp]
    \centering
    \caption{平均池化算子性能测试结果}
    \label{tab:avgpool_performance}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l|c|c|c|c|c}
        \toprule
        \textbf{测试类型} & \textbf{线程数} & \textbf{中位数(ms)} & \textbf{P99(ms)} & \textbf{加速比} & \textbf{并行效率} \\
        \midrule
        \multirow{7}{*}{\textbf{input channel并行}} 
        & 1  & 31.12 & 34.25 & 1.00× & 100.00\% \\
        & 2  & 18.26 & 21.47 & 1.70× & 85.00\% \\
        & 4  & 10.14 & 13.59 & 3.07× & 76.75\% \\
        & 8  & 6.31  & 7.48  & 4.93× & 61.63\% \\
        & 10 & 5.99  & 7.28  & 5.20× & 52.00\% \\
        & 16 & 5.22  & 8.48  & 5.96× & 37.25\% \\
        & 20 & 4.97  & 6.81  & \textbf{6.26×} & 31.30\% \\
        \midrule
        \multirow{7}{*}{\textbf{访存优化版avgpool}} 
        & 1  & 12.70 & 14.29 & 1.00× & 100.00\% \\
        & 2  & 6.91  & 7.35  & 1.84× & 92.00\% \\
        & 4  & 4.20  & 5.05  & 3.02× & 75.50\% \\
        & 8  & 2.82  & 3.49  & 4.50× & 56.25\% \\
        & 10 & 2.57  & 3.32  & 4.94× & 49.40\% \\
        & 16 & 2.50  & 3.12  & \textbf{5.08×} & 31.75\% \\
        & 20 & 2.66  & 3.11  & 4.77× & 23.85\% \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\textbf{关键发现：}池化加速比（4.45×@20线程）明显低于卷积（7.86×），并行效率下降更快，原因是算术强度低（0.31 FLOP/byte），受内存带宽限制。

\subsection{小结}

\begin{itemize}
    \item \textbf{计算密集型}（卷积）：重点是合适的并行粒度、消除False Sharing、负载均衡
    \item \textbf{访存密集型}（池化）：受内存带宽限制，需缓存优化、数据预取  
    \item \textbf{共同原则}：避免过细并行粒度、静态索引消除依赖、合理选择并行维度
\end{itemize}

\clearpage

\section{红黑排序Gauss-Seidel迭代法}

\subsection{算法原理}

Gauss-Seidel方法是求解线性方程组$Ax = b$的经典迭代方法，常用于求解泊松方程等偏微分方程的离散化系统。对于二维泊松方程：

$$
-\nabla^2 u = f, \quad (x, y) \in \Omega
$$

使用有限差分离散化后得到五点模板：

$$
\frac{u_{i-1,j} + u_{i+1,j} + u_{i,j-1} + u_{i,j+1} - 4u_{i,j}}{h^2} = f_{i,j}
$$

标准Gauss-Seidel迭代格式：

$$
u_{i,j}^{(k+1)} = \frac{1}{4}(u_{i-1,j}^{(k+1)} + u_{i+1,j}^{(k)} + u_{i,j-1}^{(k+1)} + u_{i,j+1}^{(k)} - h^2 f_{i,j})
$$

\textbf{数据依赖问题：}标准Gauss-Seidel方法在更新$u_{i,j}$时依赖于已更新的$u_{i-1,j}$和$u_{i,j-1}$，这种\textbf{读后写(RAW, Read-After-Write)依赖}使得算法难以直接并行化。

\subsection{红黑排序(Red-Black Ordering)}

红黑排序通过将网格点按照棋盘染色方式分为两类，以此打破串行数据依赖，实现并行更新。
具体而言，二维场景下的网格点按坐标关系划分，满足$i + j$为偶数的点定义为红点，$i + j$为奇数的点定义为黑点；
三维场景下则以$i + j + k$的奇偶性作为划分依据，$i + j + k$为偶数时为红点，为奇数时为黑点。这一排序方式具备关键性质，也是其打破数据依赖的核心所在：
红点的四邻点（二维场景）或六邻点（三维场景）全部为黑点，反之黑点的邻点也全部为红点，不存在同色点互为邻点的情况，由此可得出同色点之间无任何数据依赖的结论，这类同色点能够实现完全并行更新，大幅提升并行处理效率。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.3\textwidth]{figures/RB-gauss.png}
    \caption{二维红黑网格示意图}
    \label{fig:red_black_grid}
\end{figure}

迭代步骤：
\begin{enumerate}
    \item \textbf{红点更新阶段}（并行）：$u_{red}^{(k+1)} = f(\text{黑色邻点}^{(k)})$
    \item \textbf{屏障同步}：确保所有红点更新完成
    \item \textbf{黑点更新阶段}（并行）：$u_{black}^{(k+1)} = f(\text{红色邻点}^{(k+1)})$  
    \item \textbf{屏障同步}：确保所有黑点更新完成
\end{enumerate}

\subsection{OpenMP并行化与Barrier同步机制}

\subsubsection{Barrier同步原理}

\textbf{Barrier（屏障）}是一种线程同步原语，确保所有线程到达同一执行点后才能继续。
在OpenMP中，Barrier同步主要以隐式屏障和显式屏障两种形式存在：\texttt{\#pragma omp for}指令所引导的循环区域，在循环执行结束后会自动插入隐式barrier，
保障所有参与循环任务分配的线程完成各自迭代后再进入后续代码段；若在\texttt{\#pragma omp for}指令后添加nowait关键字，则会取消这一默认的隐式barrier，
允许完成任务的线程直接继续执行而无需等待其他线程；此外，用户可通过\texttt{\#pragma omp barrier}指令手动设置显式同步点，强制所有线程在该位置同步等待，
精准控制线程执行节奏。

\subsubsection{红黑GS中的Barrier需求}

在红黑高斯-塞德尔（红黑GS）迭代算法中，因迭代更新过程存在严格的数据依赖关系，
每次迭代都需要设置两个Barrier以保障计算正确性。
第一个Barrier用于确保所有红点的更新操作全部完成后，
黑点才能开始读取红点的新值进行更新，避免黑点因读取未更新的红点旧值而产生计算误差；第二个Barrier则用于确保所有黑点的更新操作结束后，
下一次迭代才能正式启动，保证每一轮迭代都基于上一轮完整的红点和黑点更新结果，避免迭代过程交叉干扰。

红黑GS迭代中必须进行上述Barrier同步控制，核心原因有三：一是数据依赖约束，
黑点更新需依赖所有红点的新值，这种跨线程依赖关系决定了必须通过Barrier等待所有红点更新完成，
否则会出现数据读取不完整问题；二是内存一致性需求，多核CPU架构下各核心有独立缓存，若不通过Barrier同步，可能导致不同核心缓存数据不一致，
引发计算结果偏差；三是迭代正确性保障，第$k+1$次迭代必须完全基于第$k$次迭代的完整结果，通过两个Barrier分别确保红点、黑点更新全部完成，才能避免迭代失真。

\subsection{小规模问题的并行困境}

\subsubsection{为什么小规模问题"越并行越慢"？}

如\autoref{tab:gs_small_scale}所示，小规模问题出现严重的并行负优化现象。

\begin{table}[!htbp]
    \centering
    \caption{小规模问题并行性能（64×64网格，10000次迭代）}
    \label{tab:gs_small_scale}
    \begin{tabular}{c|c|c|c}
    \hline
        线程数 & 执行时间(ms) & vs单线程 & 同步开销占比 \\
        \hline
        1 & 20.74 & 1.00× & 0\% \\
        2 & 288.37 & 0.07×  & $\sim$92\% \\
        4 & 653.93 & 0.03×  & $\sim$97\% \\
        8 & 1209.27 & 0.02×  & $\sim$98\% \\
        \hline
    \end{tabular}
\end{table}

\textbf{现象：}2线程比单线程慢14倍，8线程慢58倍

\textbf{根本原因分析：}

第一，计算量规模极小，其耗时远低于并行同步带来的开销。
以单次迭代、64×64网格的计算场景为例，计算量可具体量化为：64×64网格对应2048个计算点，
每个点需执行6次浮点运算（FLOPs），总计产生12288 FLOPs的计算量；
若基于3.5 GHz主频的单核处理器（每时钟周期可执行4次浮点运算）测算，单核完成该计算量的时间仅约0.88微秒。
与之形成鲜明对比的是并行同步开销：8线程下单次Barrier同步的延迟约为5至20微秒，
而每次迭代过程需执行2次Barrier同步，由此单次迭代的同步耗时可达10至40微秒。
综上可明确得出结论：同步耗时远大于计算耗时，二者差距在10倍以上。

第二，该现象也是Amdahl定律的典型体现。针对64×64网格、10000次迭代的小规模问题场景进行拆解：
可并行的计算时间为$0.88\mu\text{s} \times 10000 \times 2 = 17.6 \text{ ms}$，
而串行的同步时间为$15\mu\text{s} \times 10000 \times 2 = 300 \text{ ms}$；
据此计算串行部分占总耗时的比例为$1-P = \frac{300}{300+17.6} \approx 94\%$。
将该比例代入8线程下的Amdahl定律公式：
\[
S_p = \frac{1}{0.94 + \frac{0.06}{8}} \approx 1.06\times
\]
理论上并行加速比约为1.06倍，但实际测试得到的加速比仅为0.02倍，远低于理论值。
导致这一偏差的原因包括缓存失效、NUMA架构下的远程内存访问开销、False Sharing（伪共享）问题，以及操作系统调度过程中的抖动现象等。


\subsubsection{规模阈值分析}

根据经验法则，要求$\frac{T_{compute}}{T_{sync}} > 10$。推导规模下限（2D情况）：

设网格规模为$N \times N$，要求：

$$
\frac{3N^2 / \text{Throughput}}{2 \times T_{barrier}} > 10
$$

代入参数（3.5 GHz，4 FLOP/cycle，$T_{barrier}=15\mu s$），解得：$N > 1183$

\textbf{结论：对于2D问题，$N < 1024$时并行收益有限}

实测验证如\autoref{tab:gs_scale_threshold}所示。

\begin{table}[!htbp]
    \centering
    \caption{2D问题性能分界点（Original方法）}
    \label{tab:gs_scale_threshold}
    \begin{tabular}{c|c|c|c|c}
    \hline
        网格规模 & 2线程 & 4线程 & 8线程 & 结论 \\
        \hline
        64×64 & 0.06×  & 0.03×  & 0.02×  & 完全失败 \\
        128×128 & 0.26×  & 0.13×  & 0.05×  & 灾难性 \\
        256×256 & 0.79×  & 0.62×  & 0.27×  & 仍然负优化 \\
        512×512 & 1.66×  & 1.89×  & 1.71×  & \textbf{开始有效} \\
        1024×1024 & 1.89×  & 2.58×  & 3.30×  & 效果良好 \\
        \hline
    \end{tabular}
\end{table}

\textbf{关键发现：}
\begin{itemize}
    \item 2D临界点：512×512规模开始有效并行（与理论预测$N>1183$基本吻合）
    \item 3D优势明显：由于计算量是2D的$N$倍，$64^3$就能获得有效加速
    \item 小规模灾难：64×64和128×128的2D问题，多线程反而慢5-50倍
\end{itemize}

\subsection{两种优化方法}

本节介绍两种不同的Gauss-Seidel并行实现方法，从基础的红黑排序并行到高级的分块优化。

\subsubsection{Original方法：基础红黑排序并行}

\textbf{核心思想：}直接并行化红黑点更新

\textbf{关键设计点：}
\begin{itemize}
    \item \textbf{消除条件分支}：使用\texttt{j += 2}步长直接访问同色点，避免\texttt{if ((i+j) \% 2 == 0)}判断
    \item \textbf{自适应块大小}：小规模用小块（16-32），大规模用大块（64-128），平衡并行度和缓存
    \item \textbf{静态调度}：\texttt{schedule(static)}确保负载均匀分配
\end{itemize}

\autoref{lst:gauss_seidel_2d}展示了该方法的核心代码。

\textbf{优点：}实现简单，对大规模问题（512²以上）效果良好

\textbf{缺点：}小规模问题（<512²）同步开销占主导，严重负优化；缓存利用率不理想

\subsubsection{Tiled方法：多级缓存分块优化}

\textbf{核心思想：}针对L1/L3 Cache的两级分块策略

\textbf{优化技术：}
\begin{itemize}
    \item \textbf{两级分块}：外层L3块（128×128），内层L1块（16×16）
    \item \textbf{寄存器缓存}：显式将邻点值缓存到寄存器，减少重复内存访问
    \item \textbf{预取优化}：提前加载下一个L1块（x86平台）
\end{itemize}

\textbf{性能提升：}在中大规模问题中稳定领先Original方法10-20\%

\autoref{lst:gauss_seidel_2d_tiled}展示了该方法的核心代码。


\subsection{性能测试结果}

详细的性能测试数据见\autoref{appendix:data}。我们只对比具有实际并行收益（加速比>1）的规模，主要发现如\autoref {tab:gs_methods_summary}所示。

\begin{table}[!htbp]
    \centering
    \caption{Original vs Tiled性能对比（最优配置下的绝对时间）}
    \label{tab:gs_methods_summary}
    \begin{tabular}{c|c|c|c}
    \hline
        问题类型 & 优胜方法 & 最优时间 & 性能提升 \\
        \hline
        2D, 512×512 & \textbf{Tiled} & 86.86ms (4线程) & 快27\% \\
        2D, 1024×1024 & \textbf{Tiled} & 272.70ms (8线程) & \textbf{快43\%} \\
        2D, 2048×2048 & \textbf{Tiled} & 2810.09ms (8线程) & 快15\% \\
        \hline
        3D, $64^3$ & \textbf{Tiled} & 15.88ms (4线程) & 快14\% \\
        3D, $128^3$ & \textbf{Tiled} & 228.51ms (8线程) & 快26\% \\
        3D, $256^3$ & Original & 2374.13ms (8线程) & 快7\% \\
        3D, $512^3$ & \textbf{Tiled} & 16941.90ms (16线程) & 快3\% \\
        \hline
    \end{tabular}
\end{table}

\textbf{关键发现：}
\begin{itemize}
    \item \textbf{Tiled全面领先}：在7个测试规模中，Tiled方法赢得6场，仅在3D $256^3$输给Original
    \item \textbf{中等规模优势显著}：1024×1024（快43\%）和$128^3$（快26\%）规模下，分块优化效果最佳
    \item \textbf{超大规模仍有效}：2048×2048和$512^3$规模下，Tiled依然保持15\%和3\%的领先
    \item \textbf{唯一例外}：3D $256^3$规模下Original快7\%，可能因缓存容量与分块大小不匹配
\end{itemize}


\subsection{小结}

本节通过红黑排序Gauss-Seidel迭代法的并行化实现，揭示了迭代算法并行化的核心挑战：

\begin{itemize}
    \item \textbf{打破数据依赖}：红黑排序成功打破了串行依赖，使并行化成为可能
    \item \textbf{同步开销主导}：小规模问题中，Barrier同步时间远大于计算时间（占比>90\%），导致并行完全无效
    \item \textbf{规模阈值效应}：存在明确的并行收益阈值——2D需$N \geq 512$，3D需$N \geq 64$
    \item \textbf{分块优化显著有效}：Tiled在7个测试规模中赢得6场，中等规模提升15-43\%
    \item \textbf{最佳配置实测}：1024×1024规模下，Tiled+8线程达到272.70ms（比Original快43\%，相对单线程3.63×加速）
\end{itemize}

\clearpage
\section{三对角方程组并行求解}

\subsection{问题定义}

三对角方程组是指系数矩阵只有主对角线及其上下相邻对角线非零的线性方程组：

$$
\begin{bmatrix}
b_0 & c_0 & & & \\
a_1 & b_1 & c_1 & & \\
& a_2 & b_2 & c_2 & \\
& & \ddots & \ddots & \ddots \\
& & & a_{n-1} & b_{n-1}
\end{bmatrix}
\begin{bmatrix}
x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_{n-1}
\end{bmatrix}
=
\begin{bmatrix}
d_0 \\ d_1 \\ d_2 \\ \vdots \\ d_{n-1}
\end{bmatrix}
$$

三对角方程组广泛出现于：一维热传导方程的隐式差分格式、样条插值问题、
边界值问题的数值解以及金融工程中的期权定价模型等领域。

\subsection{Thomas算法（串行基准）}

Thomas算法（又称追赶法）是求解三对角方程组的经典串行算法，时间复杂度$O(n)$。

\textbf{算法步骤：}

\textbf{1. 前向消元（Forward Elimination）：}

$$
\gamma_i = \begin{cases}
\frac{c_0}{b_0}, & i = 0 \\
\frac{c_i}{b_i - a_i \gamma_{i-1}}, & i = 1, 2, \ldots, n-2
\end{cases}
$$

$$
\rho_i = \begin{cases}
\frac{d_0}{b_0}, & i = 0 \\
\frac{d_i - a_i \rho_{i-1}}{b_i - a_i \gamma_{i-1}}, & i = 1, 2, \ldots, n-1
\end{cases}
$$

\textbf{2. 回代（Back Substitution）：}

$$
x_i = \begin{cases}
\rho_{n-1}, & i = n-1 \\
\rho_i - \gamma_i x_{i+1}, & i = n-2, n-3, \ldots, 0
\end{cases}
$$

\textbf{数据依赖性分析：}

Thomas算法存在严重的\textbf{串行数据依赖}：
\begin{itemize}
    \item \textbf{前向消元}：第$i$步依赖第$i-1$步的$\gamma_{i-1}$和$\rho_{i-1}$
    \item \textbf{回代}：第$i$步依赖第$i+1$步的$x_{i+1}$
\end{itemize}

这种依赖链使得算法\textbf{无法直接并行化}，必须采用特殊的并行策略。

\autoref{lst:thomas_sequential}展示了Thomas算法的核心代码。

\subsection{Brugnano并行算法 - 区域分解法}

Brugnano方法通过\textbf{区域分解(Domain Decomposition)}实现并行化，核心思想是将原问题分解为多个独立的子问题。

\subsubsection{算法原理}

\textbf{四个关键步骤：}

\textbf{步骤1：区域划分} - 将方程组划分为$P$个子区域，每个子区域包含约$m = n/P$个方程。

\textbf{步骤2：局部修正Thomas算法（并行）} - 对每个子系统$k$，求解修正后的方程组，使解可表示为边界值的线性组合：

$$
x_i = \alpha_i \cdot x_{left} + \beta_i \cdot x_{right} + \gamma_i
$$

\textbf{关键点：}所有子区域可以\textbf{并行}执行修正Thomas算法，无需通信。

\textbf{步骤3：构建并求解规约系统（串行）} - 收集所有子区域边界的系数，构建规约系统（大小为$2P \times 2P$），用标准Thomas算法求解，得到所有边界值。

\textbf{步骤4：更新内部节点（并行）} - 各子区域利用已知的边界值，并行更新内部节点。

\subsubsection{实现关键技术}

\begin{itemize}
    \item \textbf{边界归一化}：将子系统边界行的主对角线归一化为1，简化线性组合表示
    \item \textbf{负载均衡}：将$n \bmod P$个多余方程分配给前几个线程，确保最大负载差异仅1个方程
    \item \textbf{同步优化}：只需2次barrier（局部求解后、规约求解后），开销可控
\end{itemize}

\autoref{lst:thomas_brugnano}展示了Brugnano方法的核心代码。

\subsection{递归倍增(Recursive Doubling)算法}

递归倍增是另一种并行化策略，通过递归地合并相邻方程实现并行。

\subsubsection{算法原理}

\textbf{核心思想：}在$\log_2 n$步中，每步将相邻的两个方程合并为一个，最终得到只包含边界的方程。

\textbf{递归步骤（第$k$步）：}

对于每个活跃方程$i$，消去与相邻方程的耦合：

$$
\begin{cases}
a_i' = -a_{i-2^k} \cdot \frac{a_i}{b_{i-2^k}} \\
b_i' = b_i - \frac{a_i \cdot c_{i-2^k}}{b_{i-2^k}} - \frac{c_i \cdot a_{i+2^k}}{b_{i+2^k}} \\
c_i' = -c_{i+2^k} \cdot \frac{c_i}{b_{i+2^k}} \\
d_i' = d_i - \frac{a_i \cdot d_{i-2^k}}{b_{i-2^k}} - \frac{c_i \cdot d_{i+2^k}}{b_{i+2^k}}
\end{cases}
$$

\textbf{并行度：}在第$k$步，有$n / 2^k$个方程可以并行处理。

\subsubsection{算法特点}

\begin{itemize}
    \item \textbf{优点}：理论上高度并行，无需显式区域划分
    \item \textbf{缺点}：计算开销大（需要$O(n \log n)$次操作），数值稳定性较差
    \item \textbf{适用场景}：大规模问题（$n > 1M$），线程数较多（>8）
\end{itemize}

\autoref{lst:thomas_recursive_doubling}展示了递归倍增算法的核心代码。

\subsection{并行化难点分析}

\subsubsection{难点1：串行依赖链}

Thomas算法的依赖链长度为$O(n)$，是最长的依赖链之一。打破依赖链的代价：
\begin{itemize}
    \item Brugnano：引入规约系统（大小$2P$），串行部分占比$\sim$5-15\%
    \item Recursive Doubling：$O(\log n)$步串行依赖，但每步计算量大
\end{itemize}

\subsubsection{难点2：规模阈值效应}

如\autoref{tab:tridiag_threshold}所示，小规模问题存在严重的并行负优化。

\begin{table}[!htbp]
    \centering
    \caption{三对角方程组规模阈值效应}
    \label{tab:tridiag_threshold}
    \begin{tabular}{c|c|c|c}
    \hline
        规模 & 串行时间 & 8线程时间 & 加速比 \\
        \hline
        8K & 0.23ms & 1.30ms (Brugnano) & 0.18×  \\
        16K & 0.24ms & 1.38ms (Brugnano) & 0.17×  \\
        128K & 1.45ms & 2.80ms (Brugnano) & 0.52×  \\
        \textbf{1M} & 10.36ms & 8.68ms (Brugnano) & \textbf{1.19× } \\
        4M & 66.07ms & 40.10ms (Brugnano) & \textbf{1.65× } \\
        \hline
    \end{tabular}
\end{table}

\textbf{关键发现：}规模 < 128K时并行完全失败，8线程慢2-6倍。1M是并行有效的临界点，8线程开始有效加速。4M规模：并行优势明显，加速比达1.65×


\subsubsection{难点3：算法本身开销}

\textbf{Brugnano方法额外开销：}
Brugnano方法存在多方面的额外开销：其一，局部数据复制操作的时间复杂度为$O(n)$；
其二，该方法中修正后的Thomas算法相比标准Thomas算法，计算量增加了约20\%；
其三，规约系统求解环节的复杂度为$O(P^2)$，当参数$P$取值较大时，这部分开销会显著增加，无法被忽略。

\textbf{Recursive Doubling额外开销：}
Recursive Doubling方法的额外开销主要体现在计算量、执行效率与数值精度三个维度：
从计算量来看，该方法的时间复杂度为$O(n \log n)$，而标准Thomas算法仅为$O(n)$，
复杂度的提升直接导致单线程场景下Recursive Doubling的执行速度比Thomas算法慢2至2.5倍；
此外，该方法存在数值误差累积的问题，由于计算过程涉及$O(\log n)$步迭代，可能会造成计算精度的损失。

\subsection{性能测试结果}

详细的性能测试数据见\ref {sec:tri_data}。主要发现如\autoref {tab:algo_perf_summary_new}所示。

\begin{table}[!htbp]
    \centering
    \caption{各数据规模下算法性能对比（最优配置）}
    \label{tab:algo_perf_summary_new}
    \begin{tabular}{c|c|c|c}
    \hline
        数据规模 & 优胜方法 & 最优时间 & 性能提升 \\
        \hline
        1M     & \textbf{RecursiveDoubling} & 8.43ms (16线程)  & 快29\% \\
        2M     & \textbf{RecursiveDoubling} & 13.27ms (16线程) & 快44\% \\
        4M     & \textbf{RecursiveDoubling} & 23.11ms (16线程) & 快51\% \\
        8M     & \textbf{RecursiveDoubling} & 39.78ms (16线程) & 快56\% \\
        16M    & \textbf{RecursiveDoubling} & 73.49ms (16线程) & 快59\% \\
        32M    & \textbf{RecursiveDoubling} & 136.66ms (20线程)& \textbf{快62\%} \\
        \hline
    \end{tabular}

\end{table}

\textbf{方法对比：}
\begin{itemize}
    \item \textbf{Sequential（串行Thomas）}：小规模问题（<1M）的唯一选择，算法最优（$O(n)$复杂度）
    \item \textbf{RecursiveDoubling（推荐）}：大规模问题（≥1M）的最佳选择，16-20线程下全面领先
    \begin{itemize}
        \item 1M规模：8.43ms vs Sequential 12.09ms（快30\%）
        \item 4M规模：23.11ms vs Sequential 46.98ms（快51\%）
        \item 32M规模：136.66ms vs Sequential 362.78ms（\textbf{快62\%}）
    \end{itemize}
    \item \textbf{Brugnano}：另一种并行方法，性能略逊于RecursiveDoubling（慢5-20\%），但实现更简单
\end{itemize}

\subsection{小结}
本节以三对角方程组的并行求解为研究对象，揭示了具有长串行依赖链的算法在并行化过程中面临的极端困难：
Thomas算法中存在的$O(n)$级串行依赖链，需借助复杂的递归策略才能实现并行化；
该类算法并行化存在极高的规模阈值，需满足$n \geq 1\text{M}$才能获得有效加速效果，
这一阈值远高于Gauss-Seidel算法所需的$512^2$规模；即便在32M规模、20线程的配置下，
该算法能达到的最佳加速比也仅为2.65倍，对应的并行效率仅13\%。

\textbf{为什么并行效率这么低？}
以4M规模的问题、RecursiveDoubling算法、16线程（该配置下达到最佳加速比2.03倍）为例进行时间分解分析：
理论最优执行时间为$46.98\text{ms} / 16 = 2.94\text{ms}$（理想无开销场景）；
实际实测的执行时间为23.11ms；由此计算出并行损失为$23.11 - 2.94 = 20.17\text{ms}$，
这部分损失占总实际执行时间的87\%。

对该并行损失的构成可做如下推测：
\begin{enumerate}
    \item \textbf{算法本身开销（最大瓶颈）}：约占并行损失的40\%。RecursiveDoubling算法在单线程下的执行时间为117.72ms，而串行的Sequential算法单线程执行时间仅为46.98ms，二者差值70.74ms即为RecursiveDoubling算法的额外计算量（相当于单线程下比Sequential慢2.5倍）；将这部分额外计算量分摊到16线程后，仍产生$70.74 / 16 = 4.42\text{ms}$的开销。
    \item \textbf{内存访问模式灾难（次要瓶颈）}：约占并行损失的35\%。Sequential算法采用顺序内存访问模式，缓存命中率超过95\%；而RecursiveDoubling算法呈现跳跃式内存访问特征（访问步长为$2^k$），导致缓存命中率不足50\%，由此带来的额外开销约为8ms。
    \item \textbf{对数级同步开销}：约占并行损失的10\%。该场景下同步次数为$\log_2(4\text{M}) \approx 22$次，单次barrier同步开销约0.02ms，总计同步开销约为$22 \times 0.02 \approx 0.44\text{ms}$，这部分开销相对较小。
    \item \textbf{线程创建与调度}：约占并行损失的5\%。
    \item \textbf{负载不均衡与其他因素}：约占并行损失的10\%。
\end{enumerate}

将三对角方程组并行求解与Gauss-Seidel算法并行化对比可知：Gauss-Seidel算法并行化的主要瓶颈为小规模场景下占比90\%以上的同步开销，或大规模场景下的内存带宽限制；而三对角方程组求解并行化的核心瓶颈则是占比40\%的算法本身开销与占比35\%的内存访问模式问题。二者的关键差异在于，三对角求解的串行算法本身已具备极优的$O(n)$时间复杂度，运行效率极高，因此打破其串行依赖链所需付出的代价相对巨大。

这一结果充分说明：\textbf{不是所有算法都适合并行化}，当串行算法本身已达到极优的$O(n)$复杂度时，对其进行并行化改造的收益往往极其有限。


%% ========================================
%% 第四章：并行算法性能总结与展望
%% ========================================
\section{并行算法性能总结与展望}

本文深入分析了三类典型计算密集型算法的并行实现与性能特征：神经网络算子（卷积、平均池化）、
红黑Gauss-Seidel迭代法和三对角方程求解。通过详尽的实测数据与理论分析，
揭示了并行计算在不同应用场景下的性能规律、瓶颈本质和实用价值。
\subsection{并行性能规律总结}

\subsubsection{规模效应的普遍性}

所有三个算法都遵循相同的规律：并行加速比随规模增长。当计算时间$T_{comp}(n) \gg T_{overhead}$时，并行才有效。数学表达为：
\begin{equation}
\text{Speedup}(n) = \frac{T_{comp}(n)}{T_{comp}(n)/p + T_{overhead}}
\label{eq:speedup_scale}
\end{equation}

\autoref{tab:scale_trend}验证了这一规律：

\begin{table}[htbp]
\centering
\caption{规模效应实测验证}
\label{tab:scale_trend}
\begin{tabular}{lrrrl}
\hline
算法 & 小规模 & 中规模 & 大规模 & 趋势 \\
\hline
池化算子 & 2.0× & 5.5× & \textbf{7.8×} & 持续增长 \\
Gauss-Seidel 2D & 0.5× & 2.5× & \textbf{4.5×} & 持续增长 \\
三对角求解 & \textbf{0.16×} & 0.51× & \textbf{1.6×} & 持续但低效 \\
\hline
\end{tabular}
\end{table}

\textbf{结论：}(1)规模效应是并行性能的第一定律；(2)不同算法的临界规模差异可达10倍；(3)临界规模主要取决于算法本身的计算复杂度。

\subsubsection{同步开销的主导作用}

在小规模问题中，同步开销主导性能。\autoref{tab:sync_overhead}展示了小规模场景下同步开销与计算时间的对比：

\begin{table}[htbp]
\centering
\caption{小规模同步开销分析}
\label{tab:sync_overhead}
\begin{tabular}{lrrrr}
\hline
算法 & 小规模计算(ms) & 同步开销(ms) & 同步/计算比 & 加速比 \\
\hline
池化 $100\times100$ & 0.1 & $\sim$1 & 10:1 & 2.0× \\
Gauss-Seidel $256^2$ & 2 & $\sim$20 & 10:1 & 0.5× \\
三对角 8K & 0.23 & $\sim$20 & \textbf{100:1} & \textbf{0.16×} \\
\hline
\end{tabular}
\end{table}

实测数据（Gauss-Seidel 2D $256^2$，10线程）显示，开销细分为：同步开销（barrier）43\%，线程创建与调度32\%，负载不均衡21\%，其他4\%。\textbf{结论：}小规模问题中同步开销占比可达90-95\%，这是并行负优化的根本原因。必须达到临界规模才能摊薄同步开销。

\subsubsection{内存带宽限制}

在大规模问题中，内存带宽成为瓶颈。\autoref{tab:memory_optimization}展示了访存优化的显著效果：

\begin{table}[htbp]
\centering
\caption{访存优化效果（池化 $1024\times1024$）}
\label{tab:memory_optimization}
\begin{tabular}{lrrr}
\hline
方法 & 时间(ms) & 加速比 & 带宽利用率 \\
\hline
串行 & 203.2 & 1.00× & $\sim$20\% \\
普通并行 10线程 & 81.6 & 2.49× & $\sim$50\% \\
\textbf{访存优化 10线程} & \textbf{26.2} & \textbf{7.75×} & \textbf{$\sim$70\%} \\
\hline
\end{tabular}
\end{table}

三对角求解遭遇访存灾难：Sequential Thomas顺序访问缓存命中率$>$95\%，单线程时间40.17ms；RecursiveDoubling跳跃访问（stride=$2^k$）缓存命中率$<$50\%，单线程时间97.20ms（2.4×慢）。

\textbf{结论：}(1)内存访问模式比算法复杂度更重要；(2)缓存友好的访问可带来3-4倍性能提升；(3)跳跃访问即使算法高效，也会导致2-3倍性能损失。

\subsubsection{线程扩展性的衰减规律}

所有算法都存在"最佳线程数"。\autoref{tab:thread_scaling}展示了不同算法的线程扩展性：

\begin{table}[htbp]
\centering
\caption{最佳线程数对比}
\label{tab:thread_scaling}
\begin{tabular}{lrrp{4cm}}
\hline
算法 & 最佳线程数 & 最佳加速比 & 超过后表现 \\
\hline
池化算子 & 16-20 & 7.8× & 几乎不变（带宽饱和） \\
Gauss-Seidel 2D & 8-10 & 4.56× & 略有下降（同步增加） \\
Brugnano & 8-10 & 1.19× & 明显下降（规约瓶颈） \\
RecursiveDoubling & 16-20 & 1.76× & 几乎不变（对数通信） \\
\hline
\end{tabular}
\end{table}

实测数据（Gauss-Seidel 3D $256^3$ Tiled+Aligned）显示：8线程达到5.03×最佳加速比（边际收益68\%），10线程降至4.61×（-8\%），16线程进一步降至3.74×（-19\%）。

基于Amdahl定律分析，假设串行比例10\%，理论8线程加速比4.7×（接近实测5.03×），但16线程理论6.4×实测仅3.74×。差距原因：同步开销随线程数增加、缓存冲突增加、NUMA效应（跨CPU访问）。

\textbf{结论：}(1)8-10线程是大多数算法的"甜点"；(2)超过16线程边际收益急剧下降；(3)除非算法有特殊优化（如对数级通信）。

\subsection{关键发现与洞察}

\subsubsection{并行不是万能药}

\autoref{tab:parallel_efficiency}汇总了实测并行效率与理论的差距：

\begin{table}[htbp]
\centering
\caption{并行效率实测vs理论}
\label{tab:parallel_efficiency}
\begin{tabular}{lrrrrr}
\hline
算法 & 最佳配置 & 理论加速比 & 实际加速比 & 并行效率 & 效率损失 \\
\hline
池化算子 & 20线程 & 20× & 7.8× & 39\% & -61\% \\
Gauss-Seidel 2D & 10线程 & 10× & 4.56× & 46\% & -54\% \\
Gauss-Seidel 3D & 8线程 & 8× & 5.03× & \textbf{63\%} & -37\% \\
三对角求解 & 16线程 & 16× & 1.76× & \textbf{11\%} & \textbf{-89\%} \\
\hline
\end{tabular}
\end{table}

\textbf{平均并行效率仅40\%}（三对角求解拉低了平均值），意味着60\%的计算资源被浪费在并行开销上。只有特定算法（如Gauss-Seidel 3D）能达到60\%+效率，大多数算法效率在30-50\%之间徘徊。

\subsubsection{算法复杂度决定并行价值}

核心洞察：\textbf{算法越慢，并行价值越高}。\autoref{tab:complexity_value}量化了这一关系：

\begin{table}[htbp]
\centering
\caption{算法复杂度与并行价值}
\label{tab:complexity_value}
\begin{tabular}{lrrr}
\hline
算法 & 复杂度 & 串行时间(ms/万元素) & 并行价值 \\
\hline
三对角求解 & $O(n)$ & $\sim$1 & 低 \\
平均池化 & $O(n^2)$ & $\sim$10 & 高 \\
Gauss-Seidel & $O(n^2k)$ & $\sim$100 & 高 \\
\hline
\end{tabular}
\end{table}

量化关系为：
\begin{equation}
\text{并行门槛} \propto \frac{\text{并行开销}}{\text{单元素计算时间}}
\label{eq:parallel_threshold}
\end{equation}

实测验证：池化并行开销1ms，单元素0.0001ms，门槛10K；三对角并行开销20ms，单元素0.0000002ms，门槛1M（100倍差距）。

\textbf{结论：}(1)不要对"本来就很快"的算法进行并行；(2)并行适合计算密集、串行执行缓慢的算法；(3)对于线性算法（如三对角），并行几乎没有价值。

\subsubsection{内存访问模式的重要性}

惊人发现：\textbf{访存优化比并行化更有效}。对于Gauss-Seidel 2D $512^2$问题：
\begin{itemize}
\item 普通并行10线程：2.5×加速
\item Tiling优化（单线程）：3.2×加速
\item Tiling+并行10线程：4.56×加速
\end{itemize}

\textbf{Tiling缓存优化带来的提升，超过了普通并行}这说明现代处理器的性能瓶颈主要在内存访问而非计算能力。

% TODO: 添加图表：访存优化效果对比柱状图

\subsection{局限性与未来工作}

\subsubsection{本研究的局限性}

\textbf{1. 测试环境单一：}仅在AMD AI 9 H 365（10核20线程）上测试，未覆盖Intel平台、ARM服务器、移动处理器等。不同架构的缓存层次、内存带宽、SIMD宽度差异可能导致不同结果。

\textbf{2. 算法代表性有限：}仅分析了三类算法，未涵盖图算法、排序算法、动态规划等其他重要并行模式。

\textbf{3. 优化技术不全面：}未使用SIMD显式向量化（仅依赖编译器自动向量化）、未实现GPU并行（CUDA/OpenCL）、未使用高级技术（任务并行、流水线）。

\textbf{4. 理论分析深度：}部分性能瓶颈基于推测而非精确测量，未使用专业性能分析工具（Intel VTune、perf），缓存行为分析不够详细。

\subsubsection{未来研究方向}

\textbf{方向1：GPU加速}

三个算法都有潜力在GPU上获得更高加速比。\autoref{tab:gpu_potential}展示了预期提升：

\begin{table}[htbp]
\centering
\caption{GPU加速潜力}
\label{tab:gpu_potential}
\begin{tabular}{lrrp{3.5cm}}
\hline
算法 & CPU最佳加速比 & GPU预期加速比 & 挑战 \\
\hline
池化算子 & 7.8× & \textbf{50-100×} & 访存模式优化 \\
Gauss-Seidel & 5.0× & \textbf{20-30×} & 红黑同步 \\
三对角求解 & 1.76× & \textbf{5-10×} & 跳跃访问 \\
\hline
\end{tabular}
\end{table}

关键技术：Shared memory优化（GPU缓存）、Warp-level并行（32线程无开销同步）、Kernel fusion（减少访存）。

\textbf{方向2：分布式并行}

对于超大规模问题（如$10^9\times10^9$ Gauss-Seidel），多机并行不可避免。挑战包括：(1)通信开销：网络延迟100-1000×内存延迟；(2)负载均衡：不规则区域划分；(3)容错性：节点故障恢复。可能方案：MPI+OpenMP混合编程、异步迭代（允许陈旧数据）、区域分解+重叠通信计算。

\textbf{方向3：自适应并行}

动态调整并行策略的智能系统：基于历史数据和问题规模自动选择串行vs并行、最佳线程数、优化方法（Tiling、Aligned等）。

\textbf{方向4：能效优化}

在能耗敏感场景（如移动设备、数据中心），性能不是唯一目标。\autoref{tab:energy_efficiency}展示能效比分析：

\begin{table}[htbp]
\centering
\caption{性能与能效权衡}
\label{tab:energy_efficiency}
\begin{tabular}{lrrr}
\hline
配置 & 性能 & 功耗 & 能效比 \\
\hline
串行 1线程 & 1.0× & 10W & 0.10 \\
并行 4线程 & 3.0× & 25W & \textbf{0.12}（最佳） \\
并行 10线程 & 4.5× & 50W & 0.09 \\
并行 20线程 & 5.0× & 80W & 0.06（最差） \\
\hline
\end{tabular}
\end{table}

\textbf{结论：}4-8线程可能是能效比最佳点。

\subsection{全文总结}

本文通过对三类典型算法的深入分析，揭示了并行计算的\textbf{真实面貌}：

\begin{enumerate}
\item \textbf{并行不是Silver Bullet：}平均并行效率仅40\%，60\%资源浪费在开销上
\item \textbf{访存比计算更重要：}Tiling优化（3×加速）$>$ 普通并行（2-3×加速）
\item \textbf{规模决定一切：}小规模问题（$<$10万元素）几乎不应并行
\item \textbf{算法特性差异巨大：}线性算法（三对角）并行价值极低，二次算法（Gauss-Seidel）并行效果好
\item \textbf{线程数的甜点：}8-10线程是大多数算法的最佳平衡点
\end{enumerate}

\textbf{最重要的启示：}在并行化之前，先问自己：(1)问题是否足够大？(2)算法是否足够慢？(3)访存是否已优化？如果三个问题的答案不全是"是"，\textbf{不要并行}。

并行计算是一门\textbf{工程艺术}，需要在理论、硬件和实践之间找到平衡。理解这些规律，才能在实际应用中做出明智的决策。
\clearpage
%% ========================================
%% 附录
%% ========================================
\appendix

\section{详细性能测试数据}
\label{appendix:data}

本附录提供各算法的完整性能测试数据，包括所有规模、线程数配置的详细结果。

\subsection{神经网络算子性能数据}

\subsubsection{卷积算子并行位置对比}
输入$150\times150\times3$，输出$32\times150\times150$，卷积核$5\times5$\\


\autoref {tab:conv_outer_dim_parallel}展示了在最外层dim维度并行的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{卷积算子最外层dim并行完整数据}
\label{tab:conv_outer_dim_parallel}
\begin{tabular}{rrrr}
\hline
线程数 & 中位数时间(ms) & 加速比 & 效率(\%) \\
\hline
1 & 18.35 & \textbf{1.00} & 100.00 \\
2 & 19.45 & 0.94 & 47.00 \\
4 & 19.72 & 0.93 & 23.25 \\
8 & 20.48 & 0.90 & 11.25 \\
10 & 20.94 & 0.88 & 8.80 \\
16 & 21.58 & 0.85 & 5.31 \\
20 & 22.11 & 0.83 & 4.15 \\
\hline
\end{tabular}
\end{table}

\autoref {tab:conv_channel_parallel}展示了在output\_channel维度并行的完整性能数据：

\begin{table}[!htbp]
\centering
\caption{卷积算子output\_channel并行完整数据}
\label{tab:conv_channel_parallel}
\begin{tabular}{rrrr}
\hline
线程数 & 中位数时间(ms) & 加速比 & 效率(\%) \\
\hline
1 & 18.20 & 1.00 & 100.00 \\
2 & 9.88 & 1.84 & 91.90 \\
4 & 5.52 & 3.30 & 82.54 \\
8 & 4.01 & 4.54 & 56.72 \\
10 & 4.05 & 4.49 & 44.91 \\
16 & 3.63 & 5.01 & 31.32 \\
20 & 3.54 & \textbf{5.14} & 25.68 \\
\hline
\end{tabular}
\end{table}

\clearpage

\autoref {tab:conv_in_channel_parallel}展示了在in\_channel维度并行的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{卷积算子in\_channel并行完整数据}
\label{tab:conv_in_channel_parallel}
\begin{tabular}{rrrr}
\hline
线程数 & 中位数时间(ms) & 加速比 & 效率(\%) \\
\hline
1 & 18.16 & 1.00 & 100.00 \\
2 & 13.90 & 1.31 & 65.50 \\
4 & 9.16 & \textbf{1.98} & 49.50 \\
8 & 10.65 & 1.71 & 21.38 \\
10 & 11.10 & 1.64 & 16.40 \\
16 & 11.70 & 1.55 & 9.69 \\
20 & 12.16 & 1.49 & 7.45 \\
\hline
\end{tabular}
\end{table}

\autoref {tab:conv_new_height_parallel}展示了在new\_height维度并行的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{卷积算子new\_height并行完整数据}
\label{tab:conv_new_height_parallel}
\begin{tabular}{rrrr}
\hline
线程数 & 中位数时间(ms) & 加速比 & 效率(\%) \\
\hline
1 & 18.27 & 1.00 & 100.00 \\
2 & 12.47 & 1.47 & 73.50 \\
4 & 9.84 & 1.86 & 46.50 \\
8 & 9.31 & \textbf{1.96} & 24.50 \\
10 & 9.54 & 1.92 & 19.20 \\
16 & 12.05 & 1.52 & 9.50 \\
20 & 12.44 & 1.47 & 7.35 \\
\hline
\end{tabular}
\end{table}

\autoref {tab:conv_new_width_parallel}展示了在new\_width维度并行的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{卷积算子new\_width并行完整数据}
\label{tab:conv_new_width_parallel}
\begin{tabular}{rrrr}
\hline
线程数 & 中位数时间(ms) & 加速比 & 效率(\%) \\
\hline
1 & 52.39 & \textbf{1.00} & 100.00 \\
2 & 331.68 & 0.16 & 8.00 \\
4 & 446.31 & 0.12 & 3.00 \\
8 & 710.87 & 0.07 & 0.88 \\
10 & 1498.34 & 0.04 & 0.40 \\
16 & 1347.83 & 0.04 & 0.25 \\
20 & 1412.46 & 0.04 & 0.20 \\
\hline
\end{tabular}
\end{table}

\autoref {tab:conv_optimized}展示了深度优化版本的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{深度优化后的卷积算子完整数据}
\label{tab:conv_optimized}
\begin{tabular}{rrrrr}
\hline
线程数 & 中位数时间(ms) & 加速比 & 效率(\%) & vs版本2提升 \\
\hline
1 & 18.58 & 1.00 & 100.00 & -2.1\% \\
2 & 9.32 & 1.99 & 99.50 & +5.7\% \\
4 & 4.93 & 3.77 & 94.25 & +10.7\% \\
8 & 3.72 & 5.00 & 62.50 & +7.2\% \\
10 & 2.82 & 6.59 & 65.90 & +30.4\% \\
16 & 2.65 & 7.00 & 43.75 & +27.0\% \\
20 & 2.36 & \textbf{7.86} & 39.30 & \textbf{33.3\%} \\
\hline
\end{tabular}
\end{table}


\subsubsection{平均池化算子访存优化效果}

\autoref{tab:avgpool_memory_full}展示了平均池化的完整测试数据（输入$1024\times1024$，池化核$3\times3$）：

\begin{table}[!htbp]
\centering
\caption{平均池化算子完整性能数据}
\label{tab:avgpool_memory_full}
\begin{tabular}{lrrrr}
\hline
版本 & 线程数 & 时间(ms) & 加速比 & vs串行 \\
\hline
input channel并行 & 1 & 31.12 & 1.00× & 0.0\% \\
 & 2 & 18.26 & 1.70× & +41.3\% \\
 & 4 & 10.14 & 3.07× & +67.4\% \\
 & 8 & 6.31 & 4.93× & +79.7\% \\
 & 10 & 5.99 & 5.20× & +80.8\% \\
 & 16 & 5.22 & 5.96× & +83.2\% \\
 & 20 & \textbf{4.97} & 6.26× & +84.0\% \\
访存优化版avgpool & 1 & 12.70 & 1.00× & 0.0\% \\
 & 2 & 6.91 & 1.84× & +45.6\% \\
 & 4 & 4.20 & 3.02× & +66.9\% \\
 & 8 & 2.82 & 4.50× & +77.8\% \\
 & 10 & 2.57 & 4.94× & +79.8\% \\
 & 16 & \textbf{2.50} & 5.08× & +80.3\% \\
 & 20 & 2.66 & 4.77× & +79.1\% \\
\hline
\end{tabular}
\end{table}

\clearpage

\subsection{Gauss-Seidel迭代法完整数据}

\subsubsection{2D红黑Gauss-Seidel性能数据（1000次迭代）}

\autoref{tab:64x64}展示了2D Gauss-Seidel在测试规模64x64下的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{测试规模 64x64}
\label{tab:64x64}
\footnotesize
\begin{tabular}{lrrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 & 效率(\%) \\
\hline
Original & 1 & 2.41 & 1.00x & 100.0 \\
 & 2 & 42.39 & 0.06x & 2.8 \\
 & 4 & 79.95 & 0.03x & 0.8 \\
 & 8 & 155.00 & 0.02x & 0.2 \\
 & 10 & 187.36 & 0.01x & 0.1 \\
 & 16 & 309.78 & 0.01x & 0.0 \\
 & 20 & 351.96 & 0.01x & 0.0 \\
Tiled & 1 & 1.97 & 1.00x & 100.0 \\
 & 2 & 36.34 & 0.05x & 2.7 \\
 & 4 & 72.32 & 0.03x & 0.7 \\
 & 8 & 149.77 & 0.01x & 0.2 \\
 & 10 & 183.66 & 0.01x & 0.1 \\
 & 16 & 286.29 & 0.01x & 0.0 \\
 & 20 & 332.27 & 0.01x & 0.0 \\
\hline
\end{tabular}
\end{table}

\autoref{tab:128x128}展示了2D Gauss-Seidel在测试规模128x128下的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{测试规模 128x128}
\footnotesize
\label{tab:128x128}
\begin{tabular}{lrrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 & 效率(\%) \\
\hline
Original & 1 & 9.54 & 1.00x & 100.0 \\
 & 2 & 36.97 & 0.26x & 12.9 \\
 & 4 & 75.65 & 0.13x & 3.2 \\
 & 8 & 191.27 & 0.05x & 0.6 \\
 & 10 & 183.41 & 0.05x & 0.5 \\
 & 16 & 291.45 & 0.03x & 0.2 \\
 & 20 & 356.62 & 0.03x & 0.1 \\
Tiled & 1 & 7.89 & 1.00x & 100.0 \\
 & 2 & 35.48 & 0.22x & 11.1 \\
 & 4 & 74.38 & 0.11x & 2.7 \\
 & 8 & 147.11 & 0.05x & 0.7 \\
 & 10 & 172.46 & 0.05x & 0.5 \\
 & 16 & 272.31 & 0.03x & 0.2 \\
 & 20 & 333.40 & 0.02x & 0.1 \\
\hline
\end{tabular}
\end{table}
\clearpage
\autoref{tab:256x256}展示了2D Gauss-Seidel在测试规模256x256下的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{测试规模 256x256}
\label{tab:256x256}
\begin{tabular}{lrrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 & 效率(\%) \\
\hline
Original & 1 & 40.99 & 1.00x & 100.0 \\
 & 2 & 51.98 & 0.79x & 39.4 \\
 & 4 & 66.19 & 0.62x & 15.5 \\
 & 8 & 149.10 & 0.27x & 3.4 \\
 & 10 & 177.71 & 0.23x & 2.3 \\
 & 16 & 283.62 & 0.14x & 0.9 \\
 & 20 & 363.70 & 0.11x & 0.6 \\
Tiled & 1 & 32.59 & 1.00x & 100.0 \\
 & 2 & 42.26 & 0.77x & 38.6 \\
 & 4 & 58.20 & 0.56x & 14.0 \\
 & 8 & 135.37 & 0.24x & 3.0 \\
 & 10 & 171.87 & 0.19x & 1.9 \\
 & 16 & 277.47 & 0.12x & 0.7 \\
 & 20 & 337.59 & 0.10x & 0.5 \\
\hline
\end{tabular}
\end{table}

\autoref{tab:512x512}展示了2D Gauss-Seidel在测试规模512x512下的完整性能数据：

\begin{table}[!htbp]
\centering
\caption{测试规模 512x512}
\label{tab:512x512}
\begin{tabular}{lrrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 & 效率(\%) \\
\hline
Original & 1 & 209.66 & 1.00x & 100.0 \\
 & 2 & 126.26 & 1.66x & 83.0 \\
 & 4 & 110.80 & 1.89x & 47.3 \\
 & 8 & 122.77 & 1.71x & 21.3 \\
 & 10 & 146.63 & 1.43x & 14.3 \\
 & 16 & 269.10 & 0.78x & 4.9 \\
 & 20 & 346.69 & 0.60x & 3.0 \\
Tiled & 1 & 156.47 & 1.00x & 100.0 \\
 & 2 & 106.88 & 1.46x & 73.2 \\
 & 4 & 86.86 & 1.80x & 45.0 \\
 & 8 & 132.38 & 1.18x & 14.8 \\
 & 10 & 154.41 & 1.01x & 10.1 \\
 & 16 & 271.39 & 0.58x & 3.6 \\
 & 20 & 344.93 & 0.45x & 2.3 \\
\hline
\end{tabular}
\end{table}
\clearpage
\autoref{tab:1024x1024}展示了2D Gauss-Seidel在测试规模1024x1024下的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{测试规模 1024x1024}
\label{tab:1024x1024}
\begin{tabular}{lrrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 & 效率(\%) \\
\hline
Original & 1 & 1285.39 & 1.00x & 100.0 \\
 & 2 & 679.25 & 1.89x & 94.6 \\
 & 4 & 498.91 & 2.58x & 64.4 \\
 & 8 & 389.11 & 3.30x & 41.3 \\
 & 10 & 406.33 & 3.16x & 31.6 \\
 & 16 & 466.60 & 2.75x & 17.2 \\
 & 20 & 543.18 & 2.37x & 11.8 \\
Tiled & 1 & 989.54 & 1.00x & 100.0 \\
 & 2 & 571.37 & 1.73x & 86.6 \\
 & 4 & 433.41 & 2.28x & 57.1 \\
 & 8 & 272.70 & 3.63x & 45.4 \\
 & 10 & 294.95 & 3.35x & 33.5 \\
 & 16 & 330.19 & 3.00x & 18.7 \\
 & 20 & 392.29 & 2.52x & 12.6 \\
\hline
\end{tabular}
\end{table}

\autoref{tab:2048x2048}展示了2D Gauss-Seidel在测试规模2048x2048下的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{测试规模 2048x2048}
\label{tab:2048x2048}
\begin{tabular}{lrrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 & 效率(\%) \\
\hline
Original & 1 & 7507.42 & 1.00x & 100.0 \\
 & 2 & 4805.16 & 1.56x & 78.1 \\
 & 4 & 3588.88 & 2.09x & 52.3 \\
 & 8 & 3221.99 & 2.33x & 29.1 \\
 & 10 & 3353.03 & 2.24x & 22.4 \\
 & 16 & 3531.92 & 2.13x & 13.3 \\
 & 20 & 3660.25 & 2.05x & 10.3 \\
Tiled & 1 & 5788.83 & 1.00x & 100.0 \\
 & 2 & 3788.45 & 1.53x & 76.4 \\
 & 4 & 2814.07 & 2.06x & 51.4 \\
 & 8 & 2810.09 & 2.06x & 25.8 \\
 & 10 & 3029.90 & 1.91x & 19.1 \\
 & 16 & 3134.95 & 1.85x & 11.5 \\
 & 20 & 3179.33 & 1.82x & 9.1 \\
\hline
\end{tabular}
\end{table}


\subsubsection{3D红黑Gauss-Seidel性能数据(100次迭代)}

\autoref{tab:64x64x64}展示了3D Gauss-Seidel在测试规模64x64x64下的完整性能数据：
\begin{table}[!htbp]
\centering
\footnotesize
\caption{测试规模 64x64x64}
\label{tab:64x64x64}
\begin{tabular}{lrrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 & 效率(\%) \\
\hline
Original & 1 & 26.70 & 1.00x & 100.0 \\
 & 2 & 19.80 & 1.35x & 67.4 \\
 & 4 & 18.08 & 1.48x & 36.9 \\
 & 8 & 18.95 & 1.41x & 17.6 \\
 & 10 & 19.49 & 1.37x & 13.7 \\
 & 16 & 29.68 & 0.90x & 5.6 \\
 & 20 & 37.57 & 0.71x & 3.6 \\
Tiled & 1 & 27.33 & 1.00x & 100.0 \\
 & 2 & 22.86 & 1.20x & 59.8 \\
 & 4 & 15.88 & 1.72x & 43.0 \\
 & 8 & 20.24 & 1.35x & 16.9 \\
 & 10 & 28.29 & 0.97x & 9.7 \\
 & 16 & 25.91 & 1.05x & 6.6 \\
 & 20 & 33.94 & 0.81x & 4.0 \\
\hline
\end{tabular}
\end{table}

\autoref{tab:128x128x128}展示了3D Gauss-Seidel在测试规模128x128x128下的完整性能数据：
\begin{table}[!htbp]
\centering
\footnotesize
\caption{测试规模 128x128x128}
\label{tab:128x128x128}
\begin{tabular}{lrrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 & 效率(\%) \\
\hline
Original & 1 & 697.82 & 1.00x & 100.0 \\
 & 2 & 396.82 & 1.76x & 87.9 \\
 & 4 & 289.87 & 2.41x & 60.2 \\
 & 8 & 288.53 & 2.42x & 30.2 \\
 & 10 & 296.93 & 2.35x & 23.5 \\
 & 16 & 283.90 & 2.46x & 15.4 \\
 & 20 & 284.61 & 2.45x & 12.3 \\
Tiled & 1 & 687.13 & 1.00x & 100.0 \\
 & 2 & 388.70 & 1.77x & 88.4 \\
 & 4 & 380.21 & 1.81x & 45.2 \\
 & 8 & 228.51 & 3.01x & 37.6 \\
 & 10 & 232.27 & 2.96x & 29.6 \\
 & 16 & 267.28 & 2.57x & 16.1 \\
 & 20 & 260.76 & 2.64x & 13.2 \\
\hline
\end{tabular}
\end{table}

\clearpage
\autoref{tab:256x256x256}展示了3D Gauss-Seidel在测试规模256x256x256下的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{测试规模 256x256x256}
\label{tab:256x256x256}
\begin{tabular}{lrrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 & 效率(\%) \\
\hline
Original & 1 & 8022.67 & 1.00x & 100.0 \\
 & 2 & 4853.07 & 1.65x & 82.7 \\
 & 4 & 3037.01 & 2.64x & 66.0 \\
 & 8 & 2374.13 & 3.38x & 42.2 \\
 & 10 & 2380.86 & 3.37x & 33.7 \\
 & 16 & 2338.09 & 3.43x & 21.4 \\
 & 20 & 2338.64 & 3.43x & 17.2 \\
Tiled & 1 & 7788.67 & 1.00x & 100.0 \\
 & 2 & 4888.85 & 1.59x & 79.7 \\
 & 4 & 3382.91 & 2.30x & 57.6 \\
 & 8 & 2533.01 & 3.07x & 38.4 \\
 & 10 & 2558.86 & 3.04x & 30.4 \\
 & 16 & 2346.54 & 3.32x & 20.7 \\
 & 20 & 2388.88 & 3.26x & 16.3 \\
\hline
\end{tabular}
\end{table}

\autoref{tab:512x512x512}展示了3D Gauss-Seidel在测试规模512x512x512下的完整性能数据：
\begin{table}[!htbp]
\centering
\caption{测试规模 512x512x512}
\label{tab:512x512x512}
\begin{tabular}{lrrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 & 效率(\%) \\
\hline
Original & 1 & 59481.10 & 1.00x & 100.0 \\
 & 2 & 31525.49 & 1.89x & 94.3 \\
 & 4 & 20381.62 & 2.92x & 73.0 \\
 & 8 & 19283.60 & 3.08x & 38.6 \\
 & 10 & 18224.77 & 3.26x & 32.6 \\
 & 16 & 17482.24 & 3.40x & 21.3 \\
 & 20 & 17714.33 & 3.36x & 16.8 \\
Tiled & 1 & 62053.14 & 1.00x & 100.0 \\
 & 2 & 34162.32 & 1.82x & 90.8 \\
 & 4 & 23006.70 & 2.70x & 67.4 \\
 & 8 & 19715.86 & 3.15x & 39.3 \\
 & 10 & 19492.76 & 3.18x & 31.8 \\
 & 16 & 16941.90 & 3.66x & 22.9 \\
 & 20 & 18065.43 & 3.43x & 17.2 \\
\hline
\end{tabular}
\end{table}
\clearpage
\subsection{三对角方程组求解完整数据}
\label{sec:tri_data}

\autoref{tab:8k}展示了三对角方程组在数据规模8k下的完整性能数据：
\begin{table}[!htbp]
\centering
\footnotesize
\caption{数据规模 8k}
\label{tab:8k}
\begin{tabular}{lrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 \\
\hline
Sequential        & 1  & 0.23  & 1.0   \\
Brugnano          & 1  & 0.22  & 1.0   \\
         & 2  & 0.74  & 0.3   \\
         & 4  & 0.97  & 0.23  \\
         & 8  & 1.3   & 0.17  \\
         & 10 & 1.41  & 0.16  \\
         & 16 & 1.8   & 0.12  \\
         & 20 & 1.96  & 0.11  \\
RecursiveDoubling & 1  & 0.26  & 1.0   \\
 & 2  & 0.77  & 0.34  \\
 & 4  & 1.34  & 0.19  \\
 & 8  & 1.83  & 0.14  \\
 & 10 & 1.54  & 0.17  \\
 & 16 & 2.52  & 0.1   \\
 & 20 & 4.04  & 0.06  \\
\hline
\end{tabular}
\end{table}

\autoref{tab:16k}展示了三对角方程组在数据规模16k下的完整性能数据：
\begin{table}[!htbp]
\centering
\footnotesize
\caption{数据规模 16k}
\label{tab:16k}
\begin{tabular}{lrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 \\
\hline
Sequential        & 1  & 0.24  & 1.0   \\
Brugnano          & 1  & 0.34  & 1.0   \\
         & 2  & 0.93  & 0.37  \\
         & 4  & 1.66  & 0.2   \\
         & 8  & 1.38  & 0.25  \\
         & 10 & 1.92  & 0.18  \\
         & 16 & 3.93  & 0.09  \\
         & 20 & 3.44  & 0.1   \\
RecursiveDoubling & 1  & 0.48  & 1.0   \\
 & 2  & 0.85  & 0.56  \\
 & 4  & 1.16  & 0.41  \\
 & 8  & 1.34  & 0.36  \\
 & 10 & 1.83  & 0.26  \\
 & 16 & 2.37  & 0.2   \\
 & 20 & 2.8   & 0.17  \\
\hline
\end{tabular}
\end{table}
\clearpage
\autoref {tab:128k}展示了三对角方程组在数据规模128k下的完整性能数据：
\begin{table}[!htbp]
\centering
\footnotesize
\caption{数据规模 128k}
\label{tab:128k}
\begin{tabular}{lrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 \\
\hline
Sequential        & 1  & 1.45  & 1.0   \\
Brugnano          & 1  & 2.2   & 1.0   \\
         & 2  & 2.26  & 0.97  \\
         & 4  & 2.3   & 0.96  \\
         & 8  & 2.8   & 0.79  \\
         & 10 & 2.82  & 0.78  \\
         & 16 & 4.1   & 0.54  \\
          & 20 & 4.34  & 0.51  \\
RecursiveDoubling & 1  & 3.27  & 1.0   \\
 & 2  & 2.54  & 1.29  \\
 & 4  & 2.21  & 1.48  \\
 & 8  & 2.95  & 1.11  \\
 & 10 & 3.55  & 0.92  \\
 & 16 & 4.32  & 0.76  \\
 & 20 & 4.21  & 0.78  \\
\hline
\end{tabular}
\end{table}

\autoref {tab:1M}展示了三对角方程组在数据规模1M下的完整性能数据：
\begin{table}[!htbp]
\centering
\footnotesize
\caption{数据规模 1M}
\label{tab:1M}
\begin{tabular}{lrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 \\
\hline
Sequential        & 1  & 12.09   & 1.00   \\
Brugnano          & 1  & 20.65   & 0.59   \\
         & 2  & 12.65   & 0.96   \\
         & 4  & 8.71    & 1.39   \\
         & 8  & 8.85    & 1.37   \\
         & 10 & 9.85    & 1.23   \\
         & 16 & 9.24    & 1.31   \\
         & 20 & 10.04   & 1.20   \\
RecursiveDoubling & 1  & 29.79   & 0.41   \\
 & 2  & 19.36   & 0.62   \\
 & 4  & 11.88   & 1.02   \\
 & 8  & 10.11   & 1.20   \\
 & 10 & 8.92    & 1.35   \\
 & 16 & 8.43    & 1.43   \\
 & 20 & 8.44    & 1.43   \\
\hline
\end{tabular}
\end{table}

\clearpage
\autoref {tab:2M}展示了三对角方程组在数据规模2M下的完整性能数据：
\begin{table}[!htbp]
\centering
\footnotesize
\caption{数据规模 2M}
\label{tab:2M}
\begin{tabular}{lrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 \\
\hline
Sequential        & 1  & 23.55   & 1.00   \\
Brugnano          & 1  & 39.33   & 0.60   \\
         & 2  & 26.84   & 0.88   \\
         & 4  & 16.72   & 1.41   \\
         & 8  & 13.96   & 1.69   \\
         & 10 & 13.80   & 1.71   \\
         & 16 & 13.75   & 1.71   \\
         & 20 & 18.44   & 1.28   \\
RecursiveDoubling & 1  & 59.20   & 0.40   \\
 & 2  & 33.13   & 0.71   \\
 & 4  & 21.83   & 1.08   \\
 & 8  & 18.07   & 1.30   \\
 & 10 & 15.36   & 1.53   \\
 & 16 & 13.27   & 1.77   \\
 & 20 & 13.99   & 1.68   \\
\hline
\end{tabular}
\end{table}

\autoref {tab:4M}展示了三对角方程组在数据规模4M下的完整性能数据：
\begin{table}[!htbp]
\centering
\footnotesize
\caption{数据规模 4M}
\label{tab:4M}
\begin{tabular}{lrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 \\
\hline
Sequential        & 1  & 46.98   & 1.00   \\
Brugnano          & 1  & 79.36   & 0.59   \\
         & 2  & 44.78   & 1.05   \\
         & 4  & 29.50   & 1.59   \\
         & 8  & 25.95   & 1.81   \\
         & 10 & 25.02   & 1.88   \\
         & 16 & 26.04   & 1.80   \\
         & 20 & 28.14   & 1.67   \\
RecursiveDoubling & 1  & 117.72  & 0.40   \\
         & 2  & 64.45   & 0.73   \\
         & 4  & 39.54   & 1.19   \\
         & 8  & 32.48   & 1.45   \\
         & 10 & 27.56   & 1.70   \\
         & 16 & 23.11   & 2.03   \\
         & 20 & 23.57   & 1.99   \\
\hline
\end{tabular}
\end{table}

\clearpage
\autoref {tab:8M}展示了三对角方程组在数据规模8M下的完整性能数据：
\begin{table}[!htbp]
\centering
\footnotesize
\caption{数据规模 8M}
\label{tab:8M}
\begin{tabular}{lrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 \\
\hline
Sequential        & 1  & 90.71   & 1.00   \\
         & 1  & 151.37  & 0.60   \\
     & 2  & 90.11   & 1.01   \\
         & 4  & 58.35   & 1.55   \\
         & 8  & 50.96   & 1.78   \\
         & 10 & 46.54   & 1.95   \\
         & 16 & 49.33   & 1.84   \\
         & 20 & 53.14   & 1.71   \\
RecursiveDoubling & 1  & 233.96  & 0.39   \\
& 2  & 128.18  & 0.71   \\
& 4  & 72.11   & 1.26   \\
& 8  & 56.35   & 1.61   \\
         & 10 & 48.20   & 1.88   \\
         & 16 & 39.78   & 2.28   \\
         & 20 & 41.85   & 2.17   \\
\hline
\end{tabular}
\end{table}

\autoref {tab:16M}展示了三对角方程组在数据规模16M下的完整性能数据：
\begin{table}[!htbp]
\centering
\footnotesize
\caption{数据规模 16M}
\label{tab:16M}
\begin{tabular}{lrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 \\
\hline
Sequential        & 1  & 181.06  & 1.00   \\
         & 1  & 300.89  & 0.60   \\
         & 2  & 167.06  & 1.08   \\
          & 4  & 110.91  & 1.63   \\
         & 8  & 94.48   & 1.92   \\
         & 10 & 92.87   & 1.95   \\
         & 16 & 93.36   & 1.94   \\
         & 20 & 94.28   & 1.92   \\
RecursiveDoubling & 1  & 466.51  & 0.39   \\
 & 2  & 248.49  & 0.73   \\
 & 4  & 139.84  & 1.29   \\
 & 8  & 104.87  & 1.73   \\
 & 10 & 93.07   & 1.95   \\
 & 16 & 73.49   & 2.46   \\
 & 20 & 80.06   & 2.26   \\
\hline
\end{tabular}
\end{table}
\clearpage
\autoref {tab:32M}展示了三对角方程组在数据规模32M下的完整性能数据： 
\begin{table}[!htbp]
\centering
\caption{数据规模 32M}
\label{tab:32M}
\begin{tabular}{lrrr}
\hline
方法 & 线程数 & 时间(ms) & 加速比 \\
\hline
Sequential        & 1  & 362.78  & 1.00   \\
Brugnano          & 1  & 664.92  & 0.55   \\
         & 2  & 343.04  & 1.06   \\
         & 4  & 207.60  & 1.75   \\
         & 8  & 182.57  & 1.99   \\
         & 10 & 175.71  & 2.06   \\
         & 16 & 179.61  & 2.02   \\
         & 20 & 178.52  & 2.03   \\
RecursiveDoubling & 1  & 933.07  & 0.39   \\
 & 2  & 495.83  & 0.73   \\
 & 4  & 280.14  & 1.29   \\
 & 8  & 203.37  & 1.78   \\
 & 10 & 180.74  & 2.01   \\
 & 16 & 142.81  & 2.54   \\
 & 20 & 136.66  & 2.65   \\
\hline
\end{tabular}
\end{table}
\clearpage
\section{关键代码实现}
\label{appendix:code}



本附录提供各算法的关键源代码实现。完整代码可在 \url{https://github.com/watney1024/iteration_final_project} 获取。

\subsection{神经网络算子实现}

\subsection{Padding实现}
\begin{lstlisting}[caption={Padding实现}, label={lst:padding}]
    Mat padd(const Mat input,int this_padding)
{
    if(this_padding == 0)
        return input;
    int new_height = input.height + 2*this_padding;
    int new_width = input.width + 2*this_padding;
    Mat new_mat(input.dim,input.channel,new_height,new_width);
    std::fill(new_mat.tensor.begin(), new_mat.tensor.end(), 0);
    for (int c = 0; c < input.channel; ++c)
    {
        for (int h = 0; h < input.height; ++h)
        {
            for (int w = 0; w < input.width; ++w)
            {
                new_mat[c * new_height * new_width + (h + this_padding) * new_width + w + this_padding] = input[c * input.height * input.width + h * input.width + w];
            }
        }
    }
    return new_mat;
}
\end{lstlisting}

\subsubsection{卷积空间并行实现}

\begin{lstlisting}[caption={卷积空间串行实现}, label={lst:conv_serial}]
    for (int d = 0; d < pd; ++d)
    { 
        for (int i = 0; i < oc; ++i)
        {
            for (int c = 0; c < pc; ++c)
            {
                for (int h = 0; h < ph; h += conv_stride[0])
                {
                    if (h + conv_kernel_size[0] > ph)
                        continue;
                    for (int w = 0; w < pw; w += conv_stride[1])
                    {
                        if (w + conv_kernel_size[1] > pw)
                            continue;
                        int index = d * pc * ph * pw + c * ph * pw + h * pw + w;
                        // std::cout << index << std::endl;
                        sum = 0;
                        for (int i = 0; i < conv_kernel_max; ++i)
                        {
                            sum += (padded_mat[index + dx[i]] * weight[weight_pos + i]);
                        }
                        output[cnt[c]++] += sum;
                    }
                }
                weight_pos += conv_kernel_max;
            }
        }
    }
    for (int i = 0; i < output.channel; ++i)
    {
        for (int j = 0; j < output.height * output.width; ++j)
        {
            output[i * output.height * output.width + j] += bias[i];
        }
    }
\end{lstlisting}

\begin{lstlisting}[caption={普通并行实现}, label={lst:conv_naive}]
    //#pragma omp parallel for
    for (int d = 0; d < padded_mat.dim; ++d)
    {
        #pragma omp parallel for
        for (int i = 0; i < output.channel; ++i)
        {
            //#pragma omp parallel for
            for (int c = 0; c < padded_mat.channel; ++c)
            {
                int weight_pos = i * padded_mat.channel * conv_kernel_max + c * conv_kernel_max;
                //#pragma omp parallel for
                for (int h = 0; h < input.height; h += conv_stride[0])
                {
                    //#pragma omp parallel for
                    for (int w = 0; w < input.width; w += conv_stride[1])
                    {
                        int index = d * padded_mat.channel * padded_mat.height * padded_mat.width + c * padded_mat.height * padded_mat.width + h * padded_mat.width + w;
                        int output_index = i * output.height * output.width + h * output.width + w;
                        for (int m = 0; m < conv_kernel_max; ++m)
                        {
                            output[output_index] += (padded_mat[index + dx[m]] * weight[weight_pos + m]);
                        }
                    }
                }
            }
        }
    }
    //#pragma omp parallel for
    for (int i = 0; i < output.channel; ++i)
    {
        for (int j = 0; j < output.height * output.width; ++j)
        {
            output[i * output.height * output.width + j] += bias[i];
        }
    }
\end{lstlisting}


\begin{lstlisting}[caption={优化后的并行实现}, label={lst:conv_optimized}]
    // 二维空间并行：任务数 = 150 × 150 = 22,500
    // 每个线程处理一个输出位置的所有通道，避免false sharing
    #pragma omp parallel for collapse(2) schedule(static)
    for (int oh = 0; oh < out_h; ++oh) {
        for (int ow = 0; ow < out_w; ++ow) {
            int h_start = oh * stride_h;
            int w_start = ow * stride_w;
            
            // 每个输出点独立计算所有输出通道
            for (int oc = 0; oc < channel_out; ++oc) {
                float sum = 0.0f;
                
                // 遍历所有输入通道
                for (int ic = 0; ic < channel_in; ++ic) {
                    const float* input_ptr = &padded_mat.tensor[
                        ic * in_h * in_w + h_start * in_w + w_start];
                    const float* weight_ptr = &weight[oc * channel_in * kernel_max + ic * kernel_max];
                    
                    // 5×5卷积核完全手动展开（25项）
                    sum += weight_ptr[0] * input_ptr[0];
                    sum += weight_ptr[1] * input_ptr[1];
                    sum += weight_ptr[2] * input_ptr[2];
                    sum += weight_ptr[3] * input_ptr[3];
                    sum += weight_ptr[4] * input_ptr[4];
                    
                    sum += weight_ptr[5] * input_ptr[in_w];
                    sum += weight_ptr[6] * input_ptr[in_w + 1];
                    sum += weight_ptr[7] * input_ptr[in_w + 2];
                    sum += weight_ptr[8] * input_ptr[in_w + 3];
                    sum += weight_ptr[9] * input_ptr[in_w + 4];
                    
                    sum += weight_ptr[10] * input_ptr[2 * in_w];
                    sum += weight_ptr[11] * input_ptr[2 * in_w + 1];
                    sum += weight_ptr[12] * input_ptr[2 * in_w + 2];
                    sum += weight_ptr[13] * input_ptr[2 * in_w + 3];
                    sum += weight_ptr[14] * input_ptr[2 * in_w + 4];
                    
                    sum += weight_ptr[15] * input_ptr[3 * in_w];
                    sum += weight_ptr[16] * input_ptr[3 * in_w + 1];
                    sum += weight_ptr[17] * input_ptr[3 * in_w + 2];
                    sum += weight_ptr[18] * input_ptr[3 * in_w + 3];
                    sum += weight_ptr[19] * input_ptr[3 * in_w + 4];
                    
                    sum += weight_ptr[20] * input_ptr[4 * in_w];
                    sum += weight_ptr[21] * input_ptr[4 * in_w + 1];
                    sum += weight_ptr[22] * input_ptr[4 * in_w + 2];
                    sum += weight_ptr[23] * input_ptr[4 * in_w + 3];
                    sum += weight_ptr[24] * input_ptr[4 * in_w + 4];
                }
                
                // 优化：直接加bias再写入，避免后续额外遍历
                output.tensor[oc * out_h * out_w + oh * out_w + ow] = sum + bias[oc];
            }
        }
    }
\end{lstlisting}
\subsubsection{平均池化并行实现}

\begin{lstlisting}[caption={普通并行实现}, label={lst:avgpool_naive}]
        for (int d = 0; d < input.dim; ++d)
    {
        #pragma omp parallel for
        for (int c = 0; c < input.channel; ++c)
        {
            //#pragma omp parallel for
            for (int oh = 0; oh < out_h; ++oh)
            {
                //#pragma omp parallel for
                for (int ow = 0; ow < out_w; ++ow)
                {
                    float sum = 0.0;
                    int count = 0;
                    //#pragma omp parallel for
                    for (int kh = 0; kh < avgp_kernel_size[0]; ++kh)
                    {
                        //#pragma omp parallel for
                        for (int kw = 0; kw < avgp_kernel_size[1]; ++kw)
                        {
                            int h = oh * avgp_stride[1] + kh;
                            int w = ow * avgp_stride[0] + kw;
                            if (h < input_h && w < input_w)
                            {
                                int index = (d * input.channel * input_h * input_w) + (c * input_h * input_w) + (h * input_w) + w;
                                sum += input[index];
                                count++;
                            }
                        }
                    }
                    output[(d * output.channel * out_h * out_w) + (c * out_h * out_w) + (oh * out_w) + ow] = sum / count;
                }
            }
        }
    }
\end{lstlisting}

优化后的并行实现(kernel\_size=2的优化版)
\begin{lstlisting}[caption={优化后的并行实现}, label={lst:avgpool_optimized}]
\label{}
    for (int d = 0; d < input.dim; ++d) {
    #pragma omp parallel for
    for (int c = 0; c < input.channel; ++c) {
        // 使用指针预计算通道偏移量
        const float* input_channel_ptr = &input.tensor[d * input.channel * input_hw + c * input_hw];
        float* output_channel_ptr = &output.tensor[d * output.channel * output_hw + c * output_hw];
            // 针对步长为2的2x2卷积核的专用优化
            for (int oh = 0; oh < out_h; ++oh) {
                for (int ow = 0; ow < out_w; ++ow) {
                    int h_start = oh * 2;
                    int w_start = ow * 2;
                    if (h_start + 1 < input_h && w_start + 1 < input_w) {
                        int idx_00 = h_start * input_w + w_start;
                        int idx_01 = idx_00 + 1;
                        int idx_10 = idx_00 + input_w;
                        int idx_11 = idx_10 + 1;
                        float sum = input_channel_ptr[idx_00] + 
                                   input_channel_ptr[idx_01] + 
                                   input_channel_ptr[idx_10] + 
                                   input_channel_ptr[idx_11];
                        output_channel_ptr[oh * out_w + ow] = sum * 0.25f; // 使用乘法代替除法（更高效）
                    } else { // 边界情况 - 使用通用处理方式
                        float sum = 0.0f;
                        int count = 0;
                        for (int kh = 0; kh < 2; ++kh) {
                            for (int kw = 0; kw < 2; ++kw) {
                                int h = h_start + kh;
                                int w = w_start + kw;
                                if (h < input_h && w < input_w) {
                                    sum += input_channel_ptr[h * input_w + w];
                                    count++;
                                }
                            }
                        }
                        output_channel_ptr[oh * out_w + ow] = sum / count;
                    }
                }
            }                 
        }
    }

\end{lstlisting}

\subsection{Gauss-Seidel迭代法实现}

\subsubsection{2D红黑排序实现}
\begin{lstlisting}[caption={2D红黑排序并行实现}, label={lst:gauss_seidel_2d}]
// gauss_seidel_2d.cpp 核心代码
void solve_parallel_redblack(/* ... */) {
    omp_set_num_threads(num_threads);
    for (int iter = 0; iter < max_iter; ++iter) {
        // === 红点更新阶段 ===
        #pragma omp for schedule(static) collapse(2) nowait
        for (int bi = 1; bi <= N; bi += tile_size) {
            for (int bj = 1; bj <= N; bj += tile_size) {
                // 块内更新红点
                for (int i = bi; i < i_end; ++i) {
                    // 关键优化：直接步长为2，无条件判断
                    int j_start = ((i + bi) % 2 == 0) ? bi : bi + 1;
                    for (int j = j_start; j < j_end; j += 2) {
                        U(i, j) = 0.25 * (U(i-1,j) + U(i+1,j) + 
                                         U(i,j-1) + U(i,j+1) + h2 * F(i-1,j-1));
                    }
                }
            }
        }
        #pragma omp barrier  // 确保所有红点更新完成
        // === 黑点更新阶段 ===
        #pragma omp for schedule(static) collapse(2) nowait
        for (int bi = 1; bi <= N; bi += tile_size) {
            for (int bj = 1; bj <= N; bj += tile_size) {
                // 块内更新黑点（步长同样为2）
                for (int i = bi; i < i_end; ++i) {
                    int j_start = ((i + bi) % 2 == 1) ? bi : bi + 1;
                    for (int j = j_start; j < j_end; j += 2) {
                        U(i, j) = 0.25 * (U(i-1,j) + U(i+1,j) + 
                                         U(i,j-1) + U(i,j+1) + h2 * F(i-1,j-1));
                    }
                }
            }
        }
        #pragma omp barrier  // 确保所有黑点更新完成
    }
}
\end{lstlisting}

\begin{lstlisting}[caption={2D红黑排序两级分块实现}, label={lst:gauss_seidel_2d_tiled}]
    // gauss_seidel_2d_tiled.cpp 核心代码
void solve_4level_tiling(/* ... */) {
    // 两级tiling参数
    const int L3_TILE = (N >= 512) ? 128 : 64;  // 外层：L3 Cache块
    const int L1_TILE = 16;                      // 内层：L1 Cache块
    for (int iter = 0; iter < max_iter; ++iter) {
        // === 红点更新：两级分块 ===
        #pragma omp for schedule(static) nowait
        for (int bi = 1; bi <= N; bi += L3_TILE) {
            int bi_end = std::min(bi + L3_TILE, N + 1);
            // L1 Cache级别的细粒度分块
            for (int ti = bi; ti < bi_end; ti += L1_TILE) {
                int ti_end = std::min(ti + L1_TILE, bi_end);
                // 预取优化：提前加载下一个L1块
                #ifdef __x86_64__
                if (ti + L1_TILE < bi_end) {
                    _mm_prefetch((const char*)&U(ti + L1_TILE, 1), _MM_HINT_T0);
                }
                #endif
                // 内核循环：访问L1块内的红点
                for (int i = ti; i < ti_end; ++i) {
                    int j_start = (i % 2 == 1) ? 1 : 2;  // 红点起始位置
                    for (int j = j_start; j <= N; j += 2) {
                        // 显式寄存器缓存邻点值
                        double reg[4];
                        reg[0] = U(i-1, j);
                        reg[1] = U(i+1, j);
                        reg[2] = U(i, j-1);
                        reg[3] = U(i, j+1);
                        U(i, j) = 0.25 * (reg[0] + reg[1] + reg[2] + 
                                reg[3] + h2 * F(i-1, j-1));
                    }
                }
            }
        }
        #pragma omp barrier
        // === 黑点更新：相同的两级分块策略 ===
        // ... (结构相同)
    }
}
\end{lstlisting}
\subsubsection{3D红黑排序实现}
\begin{lstlisting}[caption={3D红黑排序并行实现}, label={lst:gauss_seidel_3d}]
            for (int iter = 0; iter < max_iter; ++iter) {
            // 使用dynamic调度改善负载均衡
            #pragma omp for schedule(dynamic, 2) collapse(3) nowait
            for (int block_i = 1; block_i <= N; block_i += tile_size) {
                for (int block_j = 1; block_j <= N; block_j += tile_size) {
                    for (int block_k = 1; block_k <= N; block_k += tile_size) {
                        int i_end = std::min(block_i + tile_size, N + 1);
                        int j_end = std::min(block_j + tile_size, N + 1);
                        int k_end = std::min(block_k + tile_size, N + 1);
                        // ============ 改进2: 块内先红后黑，消除条件分支 ============
                        // 红点更新
                        for (int i = block_i; i < i_end; ++i) {
                            // 根据i的奇偶性确定j的起始奇偶
                            for (int j = block_j; j < j_end; ++j) {
                                // 计算k的起始值，确保(i+j+k)%2==0
                                int k_start = block_k + ((i + j + block_k) % 2 == 0 ? 0 : 1);
                                // k每次跳2，只访问红点
                                for (int k = k_start; k < k_end; k += 2) {
                                    // ============ 改进3: 使用寄存器缓存，减少内存访问 ============
                                    double u_im = U(i-1, j, k);
                                    double u_ip = U(i+1, j, k);
                                    double u_jm = U(i, j-1, k);
                                    double u_jp = U(i, j+1, k);
                                    double u_km = U(i, j, k-1);
                                    double u_kp = U(i, j, k+1);
                                    double f_val = local_h2 * F(i-1, j-1, k-1);
                                    
                                    U(i, j, k) = local_inv6 * (u_im + u_ip + u_jm + u_jp + u_km + u_kp + f_val);
                                }
                            }
                        }
                    }
                }
            }
            
            // 只在红点全部更新后同步一次
            #pragma omp barrier
            //黑点更新，结构类似
        }
\end{lstlisting}

\begin{lstlisting}[caption={3D红黑排序两级分块实现}, label={lst:gauss_seidel_3d_tiled}]
        #pragma omp parallel num_threads(num_threads)
    {
        // 线程局部变量：避免重复计算和false sharing
        double local_h2 = h2;
        double local_inv6 = inv6;
        for (int iter = 0; iter < max_iter; ++iter) {
            // ========== 红点更新 (i+j+k)%2==0 ==========
            // 使用静态调度减少调度开销，块数足够多时static更高效
            #pragma omp for schedule(static) collapse(3) nowait
            for (int block_i = 1; block_i <= N; block_i += TILE_SIZE) {
                for (int block_j = 1; block_j <= N; block_j += TILE_SIZE) {
                    for (int block_k = 1; block_k <= N; block_k += TILE_SIZE) {
                        int i_end = std::min(block_i + TILE_SIZE, N + 1);
                        int j_end = std::min(block_j + TILE_SIZE, N + 1);
                        int k_end = std::min(block_k + TILE_SIZE, N + 1);
                        // 在块内更新红点 - k循环直接跳2步
                        for (int i = block_i; i < i_end; ++i) {
                            for (int j = block_j; j < j_end; ++j) {
                                // 计算k的起始值，确保(i+j+k)%2==0
                                int k_start = block_k + ((i + j + block_k) % 2 == 0 ? 0 : 1);
                                // k每次跳2，只访问红点，无需条件判断
                                for (int k = k_start; k < k_end; k += 2) {
                                    // 寄存器缓存：减少内存访问
                                    double u_im = U(i-1, j, k);
                                    double u_ip = U(i+1, j, k);
                                    double u_jm = U(i, j-1, k);
                                    double u_jp = U(i, j+1, k);
                                    double u_km = U(i, j, k-1);
                                    double u_kp = U(i, j, k+1);
                                    double f_val = local_h2 * F(i-1, j-1, k-1);
                                    
                                    U(i, j, k) = local_inv6 * (u_im + u_ip + u_jm + u_jp + u_km + u_kp + f_val);
                                }
                            }
                        }
                    }
                }
            }
            #pragma omp barrier
            //黑点更新同理，实现方法相同
\end{lstlisting}
\subsection{三对角方程组求解实现}

\subsubsection{Sequential Thomas算法}
\begin{lstlisting}[caption={Sequential Thomas算法实现}, label={lst:thomas_sequential}]
    vector<double> thomas_solver(int n, 
                            const vector<double>& a,
                            const vector<double>& b,
                            const vector<double>& c,
                            const vector<double>& d) {
    vector<double> gamma(n, 0.0);
    vector<double> rho(n, 0.0);
    // 前向消元
    gamma[0] = c[0] / b[0];
    rho[0] = d[0] / b[0];
    for (int i = 1; i < n; i++) {
        double denom = b[i] - a[i] * gamma[i-1];
        if (i < n - 1) {
            gamma[i] = c[i] / denom;
        }
        rho[i] = (d[i] - a[i] * rho[i-1]) / denom;
    }
    // 回代
    vector<double> x(n, 0.0);
    x[n-1] = rho[n-1];
    for (int i = n - 2; i >= 0; i--) {
        x[i] = rho[i] - gamma[i] * x[i+1];
    }
    return x;
}
\end{lstlisting}

\subsubsection{Brugnano并行算法}
\begin{lstlisting}[caption={Brugnano并行算法实现}, label={lst:thomas_brugnano}]
    void thomas_brugnano(int n,
                    const vector<double>& global_a,
                    const vector<double>& global_b,
                    const vector<double>& global_c,
                    const vector<double>& global_d,
                    vector<double>& global_x,
                    int num_threads) {
    vector<int> chunk_sizes(num_threads);
    vector<int> start_indices(num_threads);
    int base = n / num_threads;
    int rem = n % num_threads;
    int cur = 0;
    for (int i = 0; i < num_threads; i++) {
        chunk_sizes[i] = base + (i < rem ? 1 : 0);
        start_indices[i] = cur;
        cur += chunk_sizes[i];
    }
    vector<vector<double>> all_coefs(num_threads, vector<double>(6));
    #pragma omp parallel num_threads(num_threads)
    {
        int tid = omp_get_thread_num();
        int start_idx = start_indices[tid];
        int m = chunk_sizes[tid];
        vector<double> local_a(m), local_b(m), local_c(m), local_d(m);
        for (int i = 0; i < m; i++) {
            int gi = start_idx + i;
            local_a[i] = global_a[gi];
            local_b[i] = global_b[gi];
            local_c[i] = global_c[gi];
            local_d[i] = global_d[gi];
        }
        if (m == 1) {
            local_d[0] /= local_b[0];
            local_a[0] = local_c[0] = 0.0;
        } else {
            modified_thomas_algorithm(m, local_a, local_b, local_c, local_d);
        }
        all_coefs[tid][0] = local_a[0];
        all_coefs[tid][1] = local_c[0];
        all_coefs[tid][2] = local_d[0];
        all_coefs[tid][3] = local_a[m-1];
        all_coefs[tid][4] = local_c[m-1];
        all_coefs[tid][5] = local_d[m-1];
        
        #pragma omp barrier
        
        #pragma omp single
        {
            int R = 2 * num_threads;
            vector<double> ra(R, 0.0), rb(R, 1.0), rc(R, 0.0), rd(R);
            for (int i = 0; i < num_threads; i++) {
                int e1 = 2 * i;
                int e2 = 2 * i + 1;
                ra[e1] = all_coefs[i][0];
                rc[e1] = all_coefs[i][1];
                rd[e1] = all_coefs[i][2];
                ra[e2] = all_coefs[i][3];
                rc[e2] = all_coefs[i][4];
                rd[e2] = all_coefs[i][5];
            }
            for (int i = 0; i < num_threads - 1; i++) {
                int curr_end = 2 * i + 1;
                int next_start = 2 * (i + 1);
                rc[curr_end] = -ra[next_start];
                ra[next_start] = -rc[curr_end];
            }
            standard_thomas_solver(R, ra, rb, rc, rd);
            for (int i = 0; i < num_threads; i++) {
                int s = start_indices[i];
                int e = s + chunk_sizes[i] - 1;
                global_x[s] = rd[2*i];
                global_x[e] = rd[2*i+1];
            }
        }
        #pragma omp barrier
        int s = start_idx;
        double d0 = global_x[s];
        double dN = global_x[s + m - 1];
        
        for (int i = 1; i < m - 1; i++) {
            global_x[s + i] = local_d[i] - local_a[i] * d0 - local_c[i] * dN;
        }
    }
}
\end{lstlisting}

\subsubsection{Recursive Doubling算法}
\begin{lstlisting}[caption={Recursive Doubling并行算法实现}, label={lst:thomas_recursive_doubling}]
    void thomas_recursive_doubling(int n,
                               const vector<double>& a,
                               const vector<double>& b,
                               const vector<double>& c,
                               const vector<double>& q,
                               vector<double>& x,
                               int num_threads) {
    vector<array<double, 4>> R_store(num_threads);
    vector<double> d(n), l(n);
    #pragma omp parallel num_threads(num_threads)
    {
        int tid = omp_get_thread_num();
        int base = n / num_threads;
        int rem = n % num_threads;
        int local_rows = (tid < rem ? base + 1 : base);
        int offset = (tid < rem ? tid * (base + 1) : rem * (base + 1) + (tid - rem) * base);
        double R00 = 1.0, R01 = 0.0;
        double R10 = 0.0, R11 = 1.0;
        for (int i = 0; i < local_rows; i++) {
            int idx = offset + i;
            double a_val = a[idx];
            double mult = (idx > 0 ? b[idx-1] * c[idx-1] : 0.0);
            
            double tmp0 = a_val * R00 - mult * R10;
            double tmp1 = a_val * R01 - mult * R11;
            R10 = R00;
            R11 = R01;
            R00 = tmp0;
            R01 = tmp1;
            
            double scale = max({abs(R00), abs(R01), abs(R10), abs(R11)});
            if (scale > 0) {
                R00 /= scale; R01 /= scale;
                R10 /= scale; R11 /= scale;
            }
        }
        R_store[tid] = {R00, R01, R10, R11};
        #pragma omp barrier
        int stages = (int)log2(num_threads);
        for (int s = 0; s < stages; s++) {
            int dist = 1 << s;
            auto my_R = R_store[tid];
            int partner = (tid >= dist ? tid - dist : tid + dist);
            auto p_R = R_store[partner];
            
            double n00 = my_R[0] * p_R[0] + my_R[1] * p_R[2];
            double n01 = my_R[0] * p_R[1] + my_R[1] * p_R[3];
            double n10 = my_R[2] * p_R[0] + my_R[3] * p_R[2];
            double n11 = my_R[2] * p_R[1] + my_R[3] * p_R[3];
            
            double scale = max({abs(n00), abs(n01), abs(n10), abs(n11)});
            if (scale > 0) {
                n00 /= scale; n01 /= scale;
                n10 /= scale; n11 /= scale;
            }
            
            R_store[tid] = {n00, n01, n10, n11};
            #pragma omp barrier
        }
        double p_sum = 0.0, q_sum = 0.0;
        for (int i = 0; i < local_rows; i++) {
            int idx = offset + i;
            p_sum += b[idx];
            q_sum += b[idx] * q[idx];
        }
        d[tid] = p_sum;
        l[tid] = q_sum;
        #pragma omp barrier
        for (int s = 0; s < stages; s++) {
            int dist = 1 << s;
            if (tid >= dist) {
                int partner = tid - dist;
                d[tid] += d[partner];
                l[tid] += l[partner];
            }
            #pragma omp barrier
        }
        for (int i = 0; i < local_rows; i++) {
            int idx = offset + i;
            x[idx] = (q[idx] - (idx > 0 ? c[idx-1] * x[idx-1] : 0.0)) / b[idx];
        }
    }
}
\end{lstlisting}


%% ========================================
%% 写在最后
%% ========================================
\section{写在最后}
\subsection{发布地址}
本项目的完整代码和文档均已开源，欢迎访问Github仓库获取最新版本：
\begin{itemize}
    \item Github: \url{https://github.com/watney1024/iteration_final_project}
\end{itemize}

%%----------- 参考文献 -------------------%%
%在reference.bib文件中填写参考文献，此处自动生成

\reference


\end{document}